{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":""},{"location":"#what-is-arcticdb","title":"What is ArcticDB?","text":"<p>ArcticDB is an embedded/serverless database engine designed to integrate with Pandas and the Python Data Science ecosystem. ArcticDB enables  you to store, retrieve and process DataFrames at scale, backed by commodity S3 storage.</p> <p>ArcticDB requires zero additional infrastructure beyond a running Python environment and access to S3 storage and can be installed in seconds.</p> <p>ArcticDB is:</p> <ul> <li>Fast: ArcticDB is incredibly fast, able to process millions of (on-disk) rows a second, and is very easy to install: <code>pip install arcticdb</code>!</li> <li>Flexible: Supporting data with and without a schema, ArcticDB is also fully compatible with streaming data ingestion. The platform is bitemporal, allowing you to see all previous versions of stored data.</li> <li>Familiar: ArcticDB is the world's simplest database, designed to be immediately familiar to anyone with prior Python and Pandas experience.</li> </ul>"},{"location":"#what-is-arcticdb-not","title":"What is ArcticDB not?","text":"<p>ArcticDB is designed for high throughput analytical workloads. It is not a transactional database and as such is not a replacement for tools such as PostgreSQL.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>The below guide covers installation, setup and basic usage. More detailed information on advanced functionality such as snapshots and parallel writers can be found in the tutorials section.</p>"},{"location":"#installation","title":"Installation","text":"<p>ArcticDB supports Python 3.6 - 3.11. To install, simply run:</p> <pre><code>pip install arcticdb\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<p>ArcticDB is a storage engine designed for S3. As a result, you must have an available S3 bucket to store data using ArcticDB. </p> <p>Storage</p> <p>ArcticDB supports any S3 API compatible storage. It has been tested against AWS S3 and storage appliances like VAST Universal Storage.</p> <p>ArcticDB also supports LMDB for local/file based storage - to use LMDB, pass an LMDB path as the URI: <code>Arctic('lmdb://path/to/desired/database')</code>.</p> <p>To get started, we can import ArcticDB and instantiate it:</p> <pre><code>&gt;&gt;&gt; from arcticdb import Arctic\n&gt;&gt;&gt; ac = Arctic(&lt;URI&gt;)\n</code></pre> <p>For more information on the format of &lt;URI&gt;, please view the docstring (<code>&gt;&gt;&gt; help(Arctic)</code>). Below we'll run through some setup examples.</p>"},{"location":"#s3-configuration-examples","title":"S3 Configuration Examples","text":"<p>There are two methods to configure S3 access. If you happen to know the access and secret key, simply connect as follows:</p> <pre><code>&gt;&gt;&gt; from arcticdb import Arctic\n&gt;&gt;&gt; ac = Arctic('s3://ENDPOINT:BUCKET?region=blah&amp;access=ABCD&amp;secret=DCBA')\n</code></pre> <p>Otherwise, you can delegate authentication to the AWS SDK (obeys standard AWS configuration options):</p> <pre><code>&gt;&gt;&gt; ac = Arctic('s3://ENDPOINT:BUCKET?aws_auth=true')\n</code></pre> <p>Same as above, but using HTTPS:</p> <pre><code>&gt;&gt;&gt; ac = Arctic('s3s://ENDPOINT:BUCKET?aws_auth=true')\n</code></pre> <p>S3</p> <p>Use <code>s3s</code> if your S3 endpoint used HTTPS</p>"},{"location":"#connecting-to-a-defined-storage-endpoint","title":"Connecting to a defined storage endpoint","text":"<p>Connect to local storage (not AWS - HTTP endpoint of s3.local) with a pre-defined access and storage key:</p> <pre><code>&gt;&gt;&gt; ac = Arctic('s3://s3.local:arcticdb-test-bucket?access=EFGH&amp;secret=HGFE')\n</code></pre>"},{"location":"#connecting-to-aws","title":"Connecting to AWS","text":"<p>Connecting to AWS with a pre-defined region:</p> <pre><code>&gt;&gt;&gt; ac = Arctic('s3s://s3.eu-west-2.amazonaws.com:arcticdb-test-bucket?aws_auth=true')\n</code></pre> <p>Note that no explicit credential parameters are given. When <code>aws_auth</code> is passed, authentication is delegated to the AWS SDK which is responsible for locating the appropriate credentials in the <code>.config</code> file or  in environment variables. </p>"},{"location":"#using-a-specific-path-within-a-bucket","title":"Using a specific path within a bucket","text":"<p>You may want to restrict access for the ArcticDB library to a specific path within the bucket. To do this, you can use the <code>path_prefix</code> parameter:</p> <pre><code>&gt;&gt;&gt; ac = Arctic('s3s://s3.eu-west-2.amazonaws.com:arcticdb-test-bucket?path_prefix=test/&amp;aws_auth=true')\n</code></pre>"},{"location":"#library-setup","title":"Library Setup","text":"<p>ArcticDB is geared towards storing many (potentially millions) of tables. Individual tables are called symbols and  are stored in collections called libraries. A single library can store an effectively unlimited number of symbols.</p> <p>Libraries must first be initialized prior to use:</p> <pre><code>&gt;&gt;&gt; ac.create_library('data')  # fixed schema - see note below\n&gt;&gt;&gt; ac.list_libraries()\n['data']\n</code></pre> <p>A library can then be retrieved:</p> <pre><code>&gt;&gt;&gt; library = ac['data']\n</code></pre> <p>ArcticDB Schemas &amp; the Dynamic Schema library option</p> <p>ArcticDB enforces a strict schema that is defined on first write. This schema defines the name, order, index type and type of each column in the DataFrame. </p> <p>If you wish to add, remove or change the type of columns via <code>update</code> or <code>append</code> options, please see the documentation for the <code>dynamic_schema</code>  option within the <code>library_options</code> parameter of the <code>create_library</code> method. Note that whether to use fixed or dynamic schemas must be set at  library creation time.</p>"},{"location":"#reading-and-writing-dataframes","title":"Reading And Writing Data(Frames)!","text":"<p>Now we have a library set up, we can get to reading and writing data! ArcticDB exposes a set of simple API primitives to enable DataFrame storage. </p> <p>Let's first look at writing a DataFrame to storage:</p> <pre><code># 50 columns, 25 rows, random data, datetime indexed. \n&gt;&gt;&gt; from datetime import datetime\n&gt;&gt;&gt; cols = ['COL_%d' % i for i in range(50)]\n&gt;&gt;&gt; df = pd.DataFrame(np.random.randint(0, 50, size=(25, 50)), columns=cols)\n&gt;&gt;&gt; df.index = pd.date_range(datetime(2000, 1, 1, 5), periods=25, freq=\"H\")\n&gt;&gt;&gt; df.head(2)\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     35     46      4      0     17     35     33     25  ...\n2000-01-01 06:00:00      9     24     18     30      0     39     43     20  ...\n</code></pre> <p>Write the DataFrame:</p> <pre><code>&gt;&gt;&gt; lib.write('test_frame', df)\nVersionedItem(symbol=test_frame,library=data,data=n/a,version=0,metadata=None,host=&lt;host&gt;)\n</code></pre> <p>ArcticDB index</p> <p>When writing Pandas DataFrames, ArcticDB supports the following index types:</p> <ul> <li><code>pandas.Index</code> containing <code>int64</code> or <code>float64</code> (or the corresponding dedicated types <code>Int64Index</code>, <code>UInt64Index</code> and <code>Float64Index</code>)</li> <li><code>RangeIndex</code> with the restrictions noted below</li> <li><code>DatetimeIndex</code></li> <li><code>MultiIndex</code> composed of above supported types</li> </ul> <p>Currently, ArcticDB only supports <code>append()</code>-ing to a <code>RangeIndex</code> with a continuing <code>RangeIndex</code> (i.e. the appending <code>RangeIndex.start</code> == <code>RangeIndex.stop</code> of the existing data and they have the same <code>RangeIndex.step</code>). If a DataFrame with a non-continuing <code>RangeIndex</code> is passed to <code>append()</code>, ArcticDB does not convert it <code>Int64Index</code> like Pandas and will produce an error.</p> <p>Also note, the \"row\" concept in <code>head()/tail()</code> refers to the physical row, not the value in the <code>pandas.Index</code>.</p> <p>Read it back:</p> <pre><code>&gt;&gt;&gt; from_storage_df = library.read('test_frame').data\n&gt;&gt;&gt; from_storage_df.head(2)\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     35     46      4      0     17     35     33     25  ...\n2000-01-01 06:00:00      9     24     18     30      0     39     43     20  ...\n</code></pre>"},{"location":"#slicing-and-filtering","title":"Slicing and Filtering","text":"<p>ArcticDB enables you to slice by row and by column. </p> <p>ArcticDB indexing</p> <p>ArcticDB will construct a full index for ordered numerical and timeseries (e.g. DatetimeIndex) Pandas indexes. This will enable optimised slicing across index entries. If the index is unsorted or not numeric, then whilst your data can be stored, row-slicing will be slower.</p>"},{"location":"#row-slicing","title":"Row-slicing","text":"<pre><code>&gt;&gt;&gt; lib.read('test_frame', date_range=(df.index[5], df.index[8])).data\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 10:00:00     43     28     36     18     10     37     31     32  ...\n2000-01-01 11:00:00     36      5     30     18     44     15     31     28  ...\n2000-01-01 12:00:00      6     34      0      5     19     41     17     15  ...\n2000-01-01 13:00:00     14     48      6      6      2      3     44     42  ...\n</code></pre>"},{"location":"#column-slicing","title":"Column slicing","text":"<pre><code>&gt;&gt;&gt; _range = (df.index[5], df.index[8])\n&gt;&gt;&gt; _columns = ['COL_30', 'COL_31']\n&gt;&gt;&gt; lib.read('test_frame', date_range=_range, columns=_columns).data\n                     COL_30  COL_31\n2000-01-01 10:00:00       7      26\n2000-01-01 11:00:00      29      18\n2000-01-01 12:00:00      36      26\n2000-01-01 13:00:00      48      42\n</code></pre>"},{"location":"#filtering","title":"Filtering","text":"<p>ArcticDB uses a Pandas-like syntax to describe how to filter data. For more details including the limitations, please view the docstring (<code>help(QueryBuilder)</code>).</p> <p>ArcticDB Filtering Philosphy &amp; Restrictions</p> <p>Note that in most cases this should be more memory efficient and performant than the equivalent Pandas operation as the processing is within the C++ storage engine and parallelized over multiple threads of execution. </p> <p>We do not intend to re-implement the entirety of the Pandas filtering/masking operations, but instead target a maximally useful subset. </p> <pre><code>&gt;&gt;&gt; _range = (df.index[5], df.index[8])\n&gt;&gt;&gt; _cols = ['COL_30', 'COL_31']\n&gt;&gt;&gt; from arcticdb import QueryBuilder\n&gt;&gt;&gt; q = QueryBuilder()\n&gt;&gt;&gt; q = q[(q[\"COL_30\"] &gt; 30) &amp; (q[\"COL_31\"] &lt; 50)]\n&gt;&gt;&gt; lib.read('test_frame', date_range=_range, columns=_cols, query_builder=q).data\n&gt;&gt;&gt;\n                     COL_30  COL_31\n2000-01-01 12:00:00      36      26\n2000-01-01 13:00:00      48      42\n</code></pre>"},{"location":"#modifications-versioning-time-travel","title":"Modifications, Versioning (time travel!)","text":"<p>ArcticDB fully supports modifying stored data via two primitives: update and append.</p>"},{"location":"#update","title":"Update","text":"<p>The update primitive enables you to overwrite a contiguous chunk of data. In the below example, we use <code>update</code> to modify 2000-01-01 05:00:00,  remove 2000-01-01 06:00:00 and insert a duplicate entry for 2000-01-01 07:00:00.</p> <pre><code># Recreate the DataFrame with new (and different!) random data, and filter to only the first and third row\n&gt;&gt;&gt; random_data = np.random.randint(0, 50, size=(25, 50))\n&gt;&gt;&gt; df = pd.DataFrame(random_data, columns=['COL_%d' % i for i in range(50)])\n&gt;&gt;&gt; df.index = pd.date_range(datetime(2000, 1, 1, 5), periods=25, freq=\"H\")\n# Filter!\n&gt;&gt;&gt; df = df.iloc[[0,2]] \n&gt;&gt;&gt; df \n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     46     24      4     20      7     32      1     18  ...\n2000-01-01 07:00:00     44     37     16     27     30      1     35     25  ...\n&gt;&gt;&gt; library.update('test_frame', df)\nVersionedItem(symbol=test_frame,library=data,data=n/a,version=1,metadata=None,host=&lt;host&gt;)\n</code></pre> <p>Now let's look at the first 2 rows in the symbol:</p> <pre><code>&gt;&gt;&gt; library.head('test_frame', 2)  # head/tail are similar to the equivalent Pandas operations\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 05:00:00     46     24      4     20      7     32      1     18  ...\n2000-01-01 07:00:00     44     37     16     27     30      1     35     25  ...\n</code></pre>"},{"location":"#append","title":"Append","text":"<p>Let's append data to the end of the timeseries:</p> <pre><code>&gt;&gt;&gt; random_data = np.random.randint(0, 50, size=(5, 50))\n&gt;&gt;&gt; df_append = pd.DataFrame(random_data, columns=['COL_%d' % i for i in range(50)])\n&gt;&gt;&gt; df_append.index = pd.date_range(datetime(2000, 1, 2, 5), periods=5, freq=\"H\")\n&gt;&gt;&gt; df_append\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-02 05:00:00     34     33      5     44     15     25      1     25  ...\n2000-01-02 06:00:00      9     39     15     18     49     47      7     45  ...\n2000-01-02 07:00:00     12     40      9     27     49     31     45      0  ...\n2000-01-02 08:00:00     43     25     39     26     13      7     20     40  ...\n2000-01-02 09:00:00      2      1     20     47     47     16     14     48  ...\n</code></pre> <p> Note the starting date of this DataFrame is after the final row written previously! </p> <p>Let's now append that DataFrame to what was written previously, and then pull back the final 7 rows from storage:</p> <pre><code>&gt;&gt;&gt; lib.append('test_frame', df_append)\nVersionedItem(symbol=test_frame,library=data,data=n/a,version=2,metadata=None,host=&lt;host&gt;)\n&gt;&gt;&gt; lib.tail('test_frame', 7).data\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-02 04:00:00      4     13      8     14     25     11     11     11  ...\n2000-01-02 05:00:00     14     41     24      7     16     10     15     36  ...\n2000-01-02 05:00:00     34     33      5     44     15     25      1     25  ...\n2000-01-02 06:00:00      9     39     15     18     49     47      7     45  ...\n2000-01-02 07:00:00     12     40      9     27     49     31     45      0  ...\n2000-01-02 08:00:00     43     25     39     26     13      7     20     40  ...\n2000-01-02 09:00:00      2      1     20     47     47     16     14     48  ...\n</code></pre> <p>The final 7 rows included the 5 rows we have just appended and the last two rows that were written previously. </p>"},{"location":"#versioning","title":"Versioning","text":"<p>You might have noticed that read calls do not return the data directly - but instead returns a VersionedItem structure. You may also have noticed that modification operations  (write, append and update) increment the version counter. ArcticDB versions all modifications, which means you can retrieve earlier versions of data (ArcticDB is a bitemporal database!):</p> <pre><code>&gt;&gt;&gt; lib.tail('test_frame', 7, as_of=0).data\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  ...\n2000-01-01 23:00:00     26     38     12     30     25     29     47     27  ...\n2000-01-02 00:00:00     12     14     42     11     44     32     19     11  ...\n2000-01-02 01:00:00     12     47      4     45     28     38     35     36  ...\n2000-01-02 02:00:00     22      0     12     48     37     11     18     14  ...\n2000-01-02 03:00:00     14     16     38     30     19     41     29     43  ...\n2000-01-02 04:00:00      4     13      8     14     25     11     11     11  ...\n2000-01-02 05:00:00     14     41     24      7     16     10     15     36  ...\n</code></pre> <p>Note the timestamps - we've read the data prior to the append operation. Please note that you can also pass a datetime into any as_of argument. </p> <p>Versioning &amp; Prune Previous</p> <p>By default, <code>write</code>, <code>append</code>, and <code>update</code> operations will not remove the previous versions. Please be aware that this will consume more space.</p> <p>This behaviour can be can be controlled via the <code>prune_previous_versions</code> keyword argument. </p>"},{"location":"error_messages/","title":"Error Messages","text":"<p>This page details the exceptions and associated error messages users are most likely to encounter, what they mean, and what (if anything) can be done to resolve the issue.</p> <p>For legacy reasons, the terms <code>symbol</code>, <code>stream</code>, and <code>stream ID</code> are used interchangeably.</p>"},{"location":"error_messages/#errors-with-numeric-error-codes","title":"Errors with numeric error codes","text":"<p>Note</p> <p>We are in the process of adding error codes to all user-facing errors. As a result, this section will expand as error codes are added to existing errors.</p>"},{"location":"error_messages/#internal-errors","title":"Internal Errors","text":"Error Code Cause Resolution 1000 An invalid date range has been passed in. ArcticDB date ranges must be in increasing order. Ensure the requested range is sorted. 1001 Invalid Argument An invalid argument has been passed in. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository. 1002 An internal ArcticDB assertion has failed. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository. 1003 ArcticDB has encountered an internal error. This error is an internal error and not expected to be exposed to the user - please create an issue on the GitHub repository."},{"location":"error_messages/#normalization-errors","title":"Normalization Errors","text":"Error Code Cause Resolution 2000 Attempting to update or append an existing type with an incompatible object type NumPy arrays or Pandas DataFrames can only be mutated by a matching type. Read the latest version of the symbol and update/append with the corresponding type. 2001 Input type cannot be converted to an ArcticDB type. Please ensure all input types match supported ArcticDB types. 2003 A write of an incompatible index type has been attempted. ArcticDB only supports defined Pandas index types. Please see the documentation for more information on what types are supported. 2004 A NumPy append is attempting to change the shape of the previous version. When storing NumPy arrays, append operations must have the same shape as the previous version."},{"location":"error_messages/#missing-data-errors","title":"Missing Data Errors","text":"Error Code Cause Resolution 3000 A missing version has been requested of a symbol. Please request a valid version - see the documentation for the <code>list_versions</code> method to enumerate existing versions."},{"location":"error_messages/#schema-error","title":"Schema Error","text":"Error Code Cause Resolution 4000 The number, type, or name of the columns has been changed. Ensure that the type and order of the columns has not changed when appending or updating the previous version. This restriction only applies when <code>Dynamic Schema</code> is disabled - if you require the columns sets to change, please enable the <code>Dynamic Schema</code> option on your library. 4001 The specified column does not exist. Please specify a valid column - use the <code>get_description</code> method to see all of the columns associated with a given symbol. 4002 The requested operation is not supported with the type of column provided. Certain operations are not supported over all column types e.g. arithmetic with the <code>QueryBuilder</code> over string columns - use the <code>get_description</code> method to see all of the columns associated with a given symbol, along with their types. 4003 The requested operation is not supported with the index type of the symbol provided. Certain operations are not supported over all index types e.g. column statistics generation with a string index - use the <code>get_description</code> method to see the index(es) associated with a given symbol, along with their types. 4004 The requested operation is not supported with pickled data. Certain operations are not supported with pickled data e.g. <code>date_range</code> filtering. If such operations are required, you must ensure that the data is of a normalizable type, such that it can be written using the <code>write</code> method, and does not require the <code>write_pickle</code> method."},{"location":"error_messages/#storage-errors","title":"Storage Errors","text":"Error Code Cause Resolution 5000 A missing key has been requested. ArcticDB has requested a key that does not exist in storage. Please ensure that you have requested a <code>symbol</code>, <code>snapshot</code>, <code>version</code>, or column statistic that exists. 5001 ArcticDB is attempting to write to an already-existing key in storage. This error is unexpected - please ensure that no other tools are writing data the same storage location that may conflict with ArcticDB."},{"location":"error_messages/#sorting-errors","title":"Sorting Errors","text":"Error Code Cause Resolution 6000 Data should be sorted for this operation. The requested operation requires data to be sorted. If this is a modification operation such as update, sort the input data. ArcticDB relies on Pandas to detect if data is sorted - you can call DataFrame.index.is_monotonic_increasing on your input DataFrame to see if Pandas believes the data to be sorted"},{"location":"error_messages/#user-input-errors","title":"User Input Errors","text":"Error Code Cause Resolution 7000 The input provided by the user is invalid in some fashion. The resolution will depend on the nature of the incorrect input, and should be explained in the associated error message."},{"location":"error_messages/#compatibility-errors","title":"Compatibility Errors","text":"Error Code Cause Resolution 8000 The version of ArcticDB being used to read the column statistics does not understand the statistics format. Update ArcticDB to (at least) the same version as that being used to create the column statistics."},{"location":"error_messages/#errors-without-numeric-error-codes","title":"Errors without numeric error codes","text":""},{"location":"error_messages/#pickling-errors","title":"Pickling errors","text":"<p>These errors relate to data being pickled, which limits the operations available. Internally, pickled symbols are stored as opaque, serialised binary blobs in the data layer. No index or column information is maintained in this serialised object which is in contrast to non-pickled data, where this information is stored in the index layer.</p> <p>Furthermore, it is not possible to partially read/update/append the data using the ArcticDB API or use the QueryBuilder with pickled symbols. </p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot append to pickled data  Cannot update pickled data A symbol has been created with the <code>write_pickle</code> method, and now <code>append</code>/<code>update</code> has been called on this symbol. Pickled data cannot be appended to or updated, due to the lack of indexing or column information in the index layer as explained above. If appending is required, the symbol must be created with <code>write</code>, and must therefore only contain normalizeable data types. Cannot delete date range of pickled data  Cannot use head/tail/row_range with pickled data, use plain read instead  Cannot filter pickled data   The data for this symbol is pickled and does not support date_range, row_range, or column queries A symbol has been created with the <code>write_pickle</code> method, and now <code>delete_data_in_range</code>/<code>head</code>/<code>tail</code>/<code>read</code> with a <code>QueryBuilder argument</code> has been called on this symbol. For reading operations, unpickling is inherently a Python-layer process. Therefore any operation that would cut down the amount of data returned to a user compared to a call to <code>read</code> with no optional parameters cannot be performed in the C++ layer, and would be no faster than calling <code>read</code> and then filtering the result down in Python."},{"location":"error_messages/#snapshot-errors","title":"Snapshot errors","text":"<p>Errors that can be encountered when creating  and deleting snapshots, or trying to read data from a specific snapshot.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Snapshot with name &lt;name&gt; already exists The <code>snapshot</code> method was called, but a snapshot with the specified name already exists. The old snapshot must first be deleted with <code>delete_snapshot</code>. Cannot snapshot version(s) that have been deleted... A <code>versions</code> dictionary was provided to the <code>snapshot</code> method, but one of the symbol-version pairs specified does not exist. The <code>list_versions</code> method can be used to see which versions of which symbols are in which snapshots. Only one of skip_symbols and versions can be set The <code>snapshot</code> method was called with both the <code>skip_symbols</code> and <code>versions</code> optional arguments set. Just specify <code>versions</code> on its own in this case."},{"location":"error_messages/#require-live-version-errors","title":"Require live version errors","text":"<p>A select few operations with ArcticDB require the symbol to exist and have at least one live version. These errors occur when this is not the case.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot update non-existent stream &lt;symbol&gt; The <code>update</code> method was called with the optional <code>upsert</code> defaulted or set to <code>False</code>, but this symbol has no live versions. If the symbol is expected to have a live version, then this is a genuine error. Otherwise, set <code>upsert</code> to <code>True</code>."},{"location":"error_messages/#date-range-related-errors","title":"Date-range related errors","text":"<p>All calls to <code>delete_data_in_range</code> and <code>update</code>, and calls to <code>read</code> using the <code>date_range</code> optional argument, require the existing data to have a sorted timestamp index. ArcticDB does not check this condition at write time.</p> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Cannot apply date range filter to symbol with non-timestamp index <code>read</code> method called with the optional <code>date_range</code> argument specified, but the symbol does not have a timestamp index. None, the <code>date_range</code> parameter does not make sense without a timestamp index. Non-contiguous rows, range search on unsorted data?... <code>read</code> method called with the optional <code>date_range</code> argument specified, and the symbol has a timestamp index, but it is not sorted. To use the <code>date_range</code> argument to <code>read</code>, the user must ensure the data is sorted on the index at write time. Delete in range will not work as expected with a non-timeseries index <code>delete_data_in_range</code> method called, but the symbol does not have a timestamp index. None, the <code>delete_data_in_range</code> method does not make sense without a timestamp index."},{"location":"error_messages/#querybuilder-errors","title":"QueryBuilder errors","text":"<p>Due to the client-only nature of ArcticDB, it is not possible to know if a <code>QueryBuilder</code> provided to <code>read</code> makes sense for the given symbol without interacting with the storage. In particular, we do not know:</p> <ul> <li>Whether a specified column exists</li> <li>What the type of the data held in a specified column is if it does exist</li> </ul> <p>All of these errors are of type <code>arcticdb.exceptions.ArcticException</code>.</p> Error messages Cause Resolution Unexpected column name A column name was specified with the <code>QueryBuilder</code> that does not exist for this symbol, and the library has dynamic schema disabled. None of the supported <code>QueryBuilder</code> operations (filtering, projections, group-bys and aggregations) make sense with non-existent columns. Non-numeric type provided to binary operation: &lt;typename&gt; Error messages like this imply that an operation that ArcticDB does not support was provided in the <code>QueryBuilder</code> argument e.g. adding two string columns together. The <code>get_description</code> method can be used to inspect the types of the columns. A full list of supported operations are provided in the <code>QueryBuilder</code> API documentation. Cannot compare &lt;typename 1&gt; to &lt;typename 2&gt; (possible categorical?) If <code>get_description</code> indicates that a column is of categorical type, and this categorical is being used to store string values, then comparisons to other strings will fail with an error message like this one. Categorical support in ArcticDB is extremely limited, but may be added in the future."},{"location":"error_messages/#exception-hierarchy","title":"Exception Hierarchy","text":"<p>ArcticDB exceptions are exposed in <code>arcticdb.exceptions</code> and sit in a hierarchy:</p> <pre><code>RuntimeError\n\u2514-- ArcticException\n    |-- ArcticNativeNotYetImplemented\n    |-- DuplicateKeyException\n    |-- MissingDataException\n    |-- NoDataFoundException\n    |-- NoSuchVersionException\n    |-- NormalizationException\n    |-- PermissionException\n    |-- SchemaException\n    |-- SortingException\n    |   \u2514-- UnsortedDataException\n    |-- StorageException\n    |-- StreamDescriptorMismatch\n    \u2514-- InternalException\n</code></pre>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>Note</p> <p>This FAQ document covers multiple topic areas - please see the contents table on the  right for more information.</p>"},{"location":"faq/#product","title":"Product","text":""},{"location":"faq/#what-is-arcticdb","title":"What is ArcticDB?","text":"<p>ArcticDB is a high performance DataFrame database built for the modern Python Data Science ecosystem. ArcticDB is an embedded database engine - which means that installing ArcticDB is as simple as installing a Python package. This also means that ArcticDB does not require any server infrastructure to function.</p> <p>ArcticDB is optimised for numerical datasets spanning millions of rows and columns, enabling you to store and retrieve massive datasets within a Pythonic, DataFrame-like API that researchers, data scientists and software engineers will find immediately familiar.</p>"},{"location":"faq/#how-does-arcticdb-differ-from-the-version-of-arctic-on-github","title":"How does ArcticDB differ from the version of Arctic on GitHub?","text":"<p>Please see the history page.</p>"},{"location":"faq/#how-does-arcticdb-differ-from-apache-parquet","title":"How does ArcticDB differ from Apache Parquet?","text":"<p>Both ArcticDB and Parquet enable the storage of columnar data without requiring additional infrastructure.</p> <p>ArcticDB however uses a custom storage format that means it offers the following functionality over Parquet:</p> <ul> <li>Versioned modifications (\"time travel\") - ArcticDB is bitemporal.</li> <li>Timeseries indexes. ArcticDB is a timeseries database and as such is optimised for slicing  and dicing timeseries data containing billions of rows.</li> <li>Data discovery - ArcticDB is built for teams. Data is structured into libraries and symbols rather  than raw filepaths.</li> <li>Support for streaming data. ArcticDB is a fully functional streaming/tick database, enabling the storage  of both batch and streaming data.</li> <li>Support for \"dynamic schemas\" - ArcticDB supports datasets with changing schemas (column sets) over time.</li> <li>Support for automatic data deduplication.</li> </ul>"},{"location":"faq/#what-sort-of-data-is-arcticdb-best-suited-to","title":"What sort of data is ArcticDB best suited to?","text":"<p>ArcticDB is an OLA(nalytical)P DBMS, rather than an OLT(ransactional)P DBMS.</p> <p>In practice, this means that ArcticDB is optimised for large numerical datasets and for queries that operate over many rows at a time.</p>"},{"location":"faq/#does-arcticdb-require-a-server","title":"Does ArcticDB require a server?","text":"<p>No. ArcticDB is a fully fledged embedded analytical database system, designed for modern cloud and on-premises object storage that does not require a server for any of the core features.</p>"},{"location":"faq/#what-languages-can-i-use-arcticdb-with","title":"What languages can I use ArcticDB with?","text":"<p>Bindings are currently only available for Python. </p>"},{"location":"faq/#how-can-i-get-started-using-arcticdb","title":"How can I get started using ArcticDB?","text":"<p>Please see our getting started guide!</p>"},{"location":"faq/#technical","title":"Technical","text":""},{"location":"faq/#does-arcticdb-use-sql","title":"Does ArcticDB use SQL?","text":"<p>No. ArcticDB enables data access and modifications with a Python API that speaks in terms of Pandas DataFrames. See the reference documentation for more details.</p>"},{"location":"faq/#does-arcticdb-de-duplicate-data","title":"Does ArcticDB de-duplicate data?","text":"<p>Yes.</p> <p>On each <code>write</code>, ArcticDB will check the previous version of the symbol that you are writing (and only this version - other symbols will not be scanned!) and skip the write of identical segments. Please keep in mind however that this is most effective when version <code>n</code> is equal to version <code>n-1</code> plus additional data at the end - and only at the end! If there is additional data inserted into the in the middle, then all segments occuring after that modification will almost certainly differ. ArcticDB segments data at fixed intervals and data is only de-duplicated if the hashes of the data segments are identical - as a result, a one row offset will prevent effective de-duplication.</p> <p>Note that this is a library configuration option that is off by default, see <code>help(LibraryOptions)</code> for details of how to enable it.</p>"},{"location":"faq/#how-does-arcticdb-enable-advanced-analytics","title":"How does ArcticDB enable advanced analytics?","text":"<p>ArcticDB is primarily focused on filtering and transfering data from storage through to memory - at which point Pandas, NumPy, or other standard analytical packages can be utilised for analytics.</p> <p>That said, ArcticDB does offer a limited set of analytical functions that are executed inside the C++ storage engine offering significant performance benefits over Pandas. For more information, see the documentation for the QueryBuilder class.</p>"},{"location":"faq/#what-does-pickling-mean","title":"What does Pickling mean?","text":"<p>ArcticDB has two means for storing data:</p> <ol> <li>ArcticDB can store your data using the Arctic On-Disk Storage Format.</li> <li>ArcticDB can Pickle your data, storing it as a giant binary blob.</li> </ol> <p>(1) is vastly more performant (i.e. reads and writes are faster), space efficient and unlocks data slicing as described in the getting started guide. There are no practical advantages to storing your data as a Pickled binary-blob - other than certain data types must be Pickled for ArcticDB to be able to store them at all!</p> <p>ArcticDB is only able to store the following data types natively:</p> <ol> <li>Pandas DataFrames</li> <li>NumPy arrays</li> <li>Integers (including timestamps - though timezone information in timestamps is removed)</li> <li>Floats</li> <li>Bools</li> <li>Strings (written as part of a DataFrame/NumPy array)</li> </ol> <p>Note that ArcticDB cannot efficiently store custom Python objects, even if inserted into a Pandas DataFrames/NumPy array.  Pickled data cannot be index or column-sliced, and neither <code>update</code> nor <code>append</code> primitives will function on pickled data. </p>"},{"location":"faq/#how-does-indexing-work-in-arcticdb","title":"How does indexing work in ArcticDB?","text":"<p>See the Getting Started page for details of supported index types.</p>"},{"location":"faq/#can-i-append-with-additional-columns-what-is-dynamic-schema","title":"Can I <code>append</code> with additional columns / What is Dynamic Schema?","text":"<p>You can <code>append</code> (or <code>update</code>) with differing column sets to symbols for which the containing library has <code>Dynamic Schema</code> enabled. See the documentation for the <code>create_library</code> method for more information.</p> <p>You can also change the type of numerical columns - for example, integers will be promoted to floats on read.</p>"},{"location":"faq/#how-does-arcticdb-segment-data","title":"How does ArcticDB segment data?","text":"<p>See On Disk Storage Format and the documentation for the <code>rows_per_segment</code> and <code>columns_per_segment</code> library configuration options for more details. </p>"},{"location":"faq/#how-does-arcticdb-handle-streaming-data","title":"How does ArcticDB handle streaming data?","text":"<p>ArcticDB has full support for streaming data workflows - including high throughput tick data pipelines. Please contact us at arcticdb@man.com for more information.</p>"},{"location":"faq/#how-does-arcticdb-handle-concurrent-writers","title":"How does ArcticDB handle concurrent writers?","text":"<p>Without a centralised server, ArcticDB does not support transactions. Instead, ArcticDB supports concurrent writers across symbols - but not to a single symbol (unless \"staging the writes\"). It is up to the writer to ensure that clients do not concurrently modify a single symbol.</p> <p>In the case of concurrent writers to a single symbol, the behaviour will be last-writer-wins. Data is not lost per se, but only the version of the last-writer will be accessible through the version chain.</p> <p>To reiterate, ArcticDB supports concurrent writers to multiple symbols, even within a single library.</p> <p>Note</p> <p>ArcticDB does support staging multiple single-symbol concurrent writes. See the documentation for <code>staged</code>. </p>"},{"location":"faq/#does-arcticdb-cache-any-data-locally","title":"Does ArcticDB cache any data locally?","text":"<p>Yes, please see the Runtime Configuration page for details.</p>"},{"location":"faq/#how-can-i-enable-detailed-logging","title":"How can I enable detailed logging?","text":"<p>Please see the Runtime Configuration page for details.</p>"},{"location":"faq/#how-can-i-tune-the-performance-of-arcticdb","title":"How can I tune the performance of ArcticDB?","text":"<p>Please see the Runtime Configuration page for details.</p>"},{"location":"faq/#does-arcticdb-support-categorical-data","title":"Does ArcticDB support categorical data?","text":"<p>ArcticDB currently offers extremely limited support for categorical data. Series and DataFrames with categorical columns can be provided to the <code>write</code> and <code>write_batch</code> methods, and will then behave as expected on <code>read</code>. However, <code>append</code> and <code>update</code> are not yet supported with categorical data, and will raise an exception if attempted. The <code>QueryBuilder</code> is also not supported with categorical data, and will either raise an exception, or give incorrect results, depending on the exact operations requested.</p>"},{"location":"faq/#how-does-arcticdb-handle-nan","title":"How does ArcticDB handle <code>NaN</code>?","text":"<p>The handling of <code>NaN</code> in ArcticDB depends on the type of the column under consideration:</p> <ul> <li>For string columns, <code>NaN</code>, as well as Python <code>None</code>, are fully supported.</li> <li>For floating-point numeric columns, <code>NaN</code> is also fully supported.</li> <li>For integer numeric columns <code>NaN</code> is not supported. A column that otherwise contains only integers will be treated as a floating point column if a <code>NaN</code> is encountered by ArcticDB, at which point the usual rules around type promotion for libraries configured with or without dynamic schema all apply as usual.</li> </ul>"},{"location":"history/","title":"History of ArcticDB","text":""},{"location":"history/#arctic","title":"Arctic","text":"<p>ArcticDB is built upon the foundations of Arctic,  an open-source, high performance datastore written in Python which utilises MongoDB as the backend  storage. Arctic has been under development within Man Group since 2012 and from its very first  release has underpinned the research and trading environments at Man Group.</p> <p>Arctic was open sourced in 2015 and has since racked up over  2,800 GitHub stars and over a million package downloads.</p> <p></p>"},{"location":"history/#to-arcticdb","title":"... to ArcticDB!","text":"<p>In 2018, Man Group embarked on a ground-up rewrite in order to improve upon some of the  foundational limitations of Arctic. We called this version ArcticDB, and is differentiated from Arctic in three main ways:</p> <ol> <li>ArcticDB does not depend on Mongo. Instead, ArcticDB is designed to work with consumer grade S3 - on prem or in the cloud.</li> <li>ArcticDB is written in C++, enabling signficant performance improvements. ArcticDB is an order of magnitude faster than Arctic whilst being vastly easier to set up and get started with. </li> <li>ArcticDB unifies streaming and batch workflows behind the same easy to use, consistent API. </li> </ol> <p>All in all, ArcticDB offers tremendous, scalable and portable performance with the same intuitive  Python and Pandas-centric API as Arctic. Behind the scenes, it utilises a custom C++ storage engine, along with  modern S3-compatible object storage. Both bulk and streaming data workflows have unified APIs, offering a bi-temporal view of data history with no performance penalty. </p> <p>ArcticDB's versatility and ease-of-use have made it the database of choice for all front-office timeseries analysis at Man Group.</p> <p>For more information on how ArcticDB is licensed, please see the licensing FAQ.</p>"},{"location":"lib_config/","title":"Library Configuration","text":"<p>When creating a new library, all configuration options will default to those specified in the <code>LibraryOptions</code> documentation defaults, unless overridden by an explicitly specified <code>LibraryOptions</code> object passed in to the call to <code>create_library</code>.</p> <p>Note that currently library configuration options cannot be changed once the library has been created. The ability to modify library options for existing libraries will be added soon.</p>"},{"location":"licensing/","title":"Licensing &amp; Commercial FAQs","text":"<p>ArcticDB is licensed under the Business Source License 1.1 (BSL), reverting to an Apache 2.0 License after a two-year term. </p>"},{"location":"licensing/#is-arcticdb-free-to-use","title":"Is ArcticDB free to use?","text":"<p>All versions of ArcticDB are free to use for non-production usage - except if being used to provide an offering that allows third parties (other than your employees and contractors) to operate a database.</p> <p>Under the BSL, ArcticDB cannot be used for production usage. For more information on what constitutes production usage, please see the MariaDB BSL FAQ. </p> <p>Once a version of ArcticDB has converted to the Apache 2.0 license, it may be used for production usage without acquiring an additional license.</p>"},{"location":"licensing/#license-conversion-timeline","title":"License conversion timeline","text":"ArcticDB version License Converts to Apache 2.0 1.0 BSL March 16, 2025"},{"location":"licensing/#production-usage-of-a-bsl-licensed-version","title":"Production usage of a BSL-licensed version","text":"<p>To use ArcticDB for production use, please contact us to purchase an ArcticDB Pro license. </p>"},{"location":"licensing/#can-i-contribute-to-arcticdb","title":"Can I contribute to ArcticDB?","text":"<p>Yes! Contributions are welcome - please see our GitHub readme for more information.</p>"},{"location":"licensing/#is-arcticdb-part-of-man-group","title":"Is ArcticDB part of Man Group?","text":"<p>Yes - ArcticDB is provided by Man Group.</p>"},{"location":"runtime_config/","title":"Runtime Configuration","text":"<p>ArcticDB features a variety of options that can be tuned at runtime. This page details the most commonly modified options, and how to configure them.</p>"},{"location":"runtime_config/#configuration-methods","title":"Configuration methods","text":"<p>All of the integer options detailed on this page can be configured using the following two methods. All of the options listed on this page are integer options except for log levels, which will be explained in their own section.</p>"},{"location":"runtime_config/#in-code","title":"In code","text":"<p>For integer options, the following code snippet demonstrates how to set values in code:</p> <pre><code>from arcticdb_ext import set_config_int\nset_config_int(setting, value)\n</code></pre> <p>where <code>setting</code> is a string containing the setting name (e.g. <code>VersionMap.ReloadInterval</code>), and <code>value</code> is an int to set the option to.</p>"},{"location":"runtime_config/#environment-variables","title":"Environment variables","text":"<p>For integer options, environment variables can be used to set options as follows:</p> <pre><code>ARCTICDB_&lt;setting&gt;_int=&lt;value&gt;\n</code></pre> <p>e.g. <code>ARCTICDB_VersionMap_ReloadInterval_int=0</code>. Note that <code>.</code> characters in setting names are replaced with underscores when setting them by environment variables.</p>"},{"location":"runtime_config/#priority","title":"Priority","text":"<p>If both the environment variable is set, and <code>set_config_int</code> is called, then the latter takes priority.</p>"},{"location":"runtime_config/#reactivity","title":"Reactivity","text":"<p>Configuration options are read once when the <code>Library</code> instance is created, and are not monitored after that point, so all options should be configured before the <code>Library</code> object is constructed.</p>"},{"location":"runtime_config/#configuration-options","title":"Configuration options","text":""},{"location":"runtime_config/#versionmapreloadinterval","title":"VersionMap.ReloadInterval","text":"<p>ArcticDB library instances maintain a short-lived cache containing what it believes is the latest version for every encountered symbol.  This cache is invalidated after 5 seconds by default.</p> <p>As a result of this caching, it is theoretically possible for two independent library instances to disagree as to what the latest version of a symbol is for a short period of time.</p> <p>This caching is designed to reduce load on storage - if this is not a concern it can be safely disabled by setting this option to <code>0</code>.</p> <p>Other than this, there is no client-side caching in ArcticDB.</p>"},{"location":"runtime_config/#symbollistmaxdelta","title":"SymbolList.MaxDelta","text":"<p>The symbol list cache is compacted when there are more than <code>SymbolList.MaxDelta</code> objects on disk in the symbol list cache.</p> <p>The default is 500.</p>"},{"location":"runtime_config/#s3storagedeletebatchsize","title":"S3Storage.DeleteBatchSize","text":"<p>The S3 API supports the <code>DeleteObjects</code> method, whereby a single HTTP request can be used to delete multiple objects. This parameter can be used to control how many objects are requested to be deleted at a time.</p> <p>The default is 1000.</p>"},{"location":"runtime_config/#versionstorenumcputhreads-and-versionstorenumiothreads","title":"VersionStore.NumCPUThreads and VersionStore.NumIOThreads","text":"<p>ArcticDB uses two threadpools in order to manage computational resources:</p> <ul> <li>CPU - used for CPU intensive tasks such as decompressing or filtering data</li> <li>IO - used to read/write data from/to the underlying storage</li> </ul> <p>By default, ArcticDB attempts to infer sensible sizes for these threadpools based on the number of cores* available on the host machine. The CPU threadpool size defaults to the number of cores available on the host machine, while the IO threadpool size defaults to x1.5 the CPU threadpool size. If these defaults are not suitable for a particular use case, these threadpool sizes can be set directly .</p> <p>If only <code>NumCPUThreads</code> is set, <code>NumIOThreads</code> will still default to x1.5 <code>NumCPUThreads</code>.</p> <p>*On Linux machines, this core count takes cgroups into account. In particular, this means that CPU limits are respected in processes running in Kubernetes.</p>"},{"location":"runtime_config/#logging-configuration","title":"Logging configuration","text":"<p>ArcticDB has multiple log streams, and the verbosity of each can be configured independently. The available streams are visible in the source code, although the most commonly useful logs are in:</p> <ul> <li><code>version</code> - contains information about versions being read, created, or destroyed, and traversal of the version layer linked list</li> <li><code>storage</code> - contains information about individual operations that interact with the storage device (read object, write object, delete object, etc)</li> </ul> <p>The available log levels in decreasing order of verbosity are are <code>TRACE</code>, <code>DEBUG</code>, <code>INFO</code>, <code>WARN</code>, <code>ERROR</code>, <code>CRITICAL</code>, <code>OFF</code>. By default, all streams are set to the <code>INFO</code> level.</p> <p>There are two ways to configure log levels. The first is via environment variables e.g. <code>ARCTICDB_version_loglevel=DEBUG</code>. All of the streams can be configured together via <code>ARCTICDB_all_loglevel=DEBUG</code>. The second is in code by calling <code>set_log_level</code> from the <code>arcticdb.config</code> module. This takes two optional arguments:</p> <ul> <li><code>default_level</code> - the default level for all streams. Should be a string such as <code>\"DEBUG\"</code></li> <li><code>specific_log_levels</code> - a dictionary from stream names to log levels used to override the default such as <code>{\"version\": \"DEBUG\"\"}</code>.</li> </ul> <p>If both environment variables are set, and <code>set_log_level</code> is called, then the latter takes priority.</p> <p>Note that all logging from ArcticDB goes to <code>stderr</code>, and this is not configurable.</p> <p>S3 logging can also be enabled by setting the environment variable <code>ARCTICDB_AWS_LogLevel_int=6</code>, which will output all S3 logs to a file in the present working directory. See the AWS documentation for more details.</p>"},{"location":"notebooks/","title":"Creating shareable notebooks","text":"<p>To make the notebooks in this folder portable encode any images as PNG and in base64 and include as a Data URI.</p> <p>Issues with other approaches, - Linking to images     Given we want users to open our demo notebooks in their preferred environment, a linked image may not be displayed, and if it is it requires us to maintain hosting of that image. - Embedded attachments     This doesn't seem to be well supported by third party viewers, e.g. in Github and Google Colab. - Embedded SVG files     Doesn't work in Github in a Markdown cell</p>"},{"location":"notebooks/#png-example","title":"PNG example","text":"<p>It's good to keep the images small for this, modern browsers support large \"Data URLs\" but it can be annoying as a user users, if they open the cell it will become very large and hard to scroll, etc.</p> <p>The resulting Markdown cells should contain an image tag that looks something like this small example. <pre><code>&lt;img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAUAAAAFCAYAAACNbyblAAAAHElEQVQI12P4//8/w38GIAXDIBKE0DHxgljNBAAO9TXL0Y4OHwAAAABJRU5ErkJggg==\" alt=\"Red dot\" /&gt;\n</code></pre></p>"},{"location":"notebooks/#base64-encoding","title":"Base64 encoding","text":"<p>It's important to disable line wrapping with <code>-w0</code>, Google Colab doesn't handle newlines within base64 encodings, though github does. </p> <pre><code>base64 -w0 images/ArcticDBLogo.svg\n</code></pre>"},{"location":"technical/architecture/","title":"Architecture","text":""},{"location":"technical/architecture/#overview","title":"Overview","text":"<p>ArcticDB is deployed as a shared library, using PyBind for operability between CPython and the core database engine which is written in C++.</p> <p>Users interact with the C++ storage engine, and therefore the storage itself, via the Python/C++ bindings. The engine transforms Python objects which are typically DataFrames, Series or numpy arrays to and from its internal columnar structure. The data is then tiled, indexed, compressed, and written to storage. The storage format is tailor-designed for the storage and retrieval of dense and sparse timeseries data. </p> <p>Please note that there is no required server component.</p> <p></p>"},{"location":"technical/architecture/#arcticdb-dataflow","title":"ArcticDB DataFlow","text":"<p>The below diagram visualises the flow of data through ArcticDB from source to storage and back again:</p> <p></p> <p>For more information on the storage format, please see On-Disk Storage.</p>"},{"location":"technical/contributing/","title":"Contribution Licensing","text":"<p>Since this project is distributed under the terms of the BSL license, contributions that you make are licensed under the same terms. For us to be able to accept your contributions, we will need explicit confirmation from you that you are able and willing to provide them under these terms, and the mechanism we use to do this is the ArcticDB Individual Contributor License Agreement.</p> <p>Individuals - To participate under these terms, please include the following line as the last line of the commit message for each commit in your contribution. You must use your real name (no pseudonyms, and no anonymous contributions).</p> <pre><code>Signed-Off By: Random J. Developer &lt;random@developer.example.org&gt;. By including this sign-off line I agree to the terms of the Contributor License Agreement.\n</code></pre> <p>Corporations - For corporations who wish to make contributions to ArcticDB, please contact arcticdb@man.com and we will arrange for the CLA to be sent to the signing authority within your corporation.</p>"},{"location":"technical/contributing/#docker-quickstart","title":"Docker Quickstart","text":"<p>This quickstart builds a release using build dependencies from vcpkg. ArcticDB releases on PyPi use vcpkg dependencies in the manner as described below.</p> <p>Note the below instructions will build a Linux X86_64 release.</p>"},{"location":"technical/contributing/#1-start-the-arcticdb-build-docker-image","title":"1) Start the ArcticDB build docker image","text":"<p>Run in a Linux terminal:</p> <pre><code>docker pull ghcr.io/man-group/cibuildwheel_manylinux:2.12.1-3a897\ndocker run -it ghcr.io/man-group/cibuildwheel_manylinux:2.12.1-3a897\n</code></pre> <p>:warning: The below instructions do not have to be run in the provided docker image. They can be run against any Python installation on any Linux distribution as long as the basic build dependencies are available.</p> <p>If running outside the provided docker image, please change <code>/opt/python/cp39-cp39/bin/python3</code> in the examples below to an appropriate path for Python.</p>"},{"location":"technical/contributing/#2-check-out-arcticdb-including-submodules","title":"2) Check out ArcticDB including submodules","text":"<pre><code>cd\ngit clone https://github.com/man-group/ArcticDB.git\ncd ArcticDB\ngit submodule init &amp;&amp; git submodule update\n</code></pre>"},{"location":"technical/contributing/#3-kick-off-the-build","title":"3) Kick off the build","text":"<pre><code>MY_PYTHON=/opt/python/cp39-cp39/bin/python3  # change if outside docker container/building against a different python\n$MY_PYTHON -m pip install -U pip setuptools wheel grpcio-tools\nARCTIC_CMAKE_PRESET=skip $MY_PYTHON setup.py develop\n# Change the below Python_EXECUTABLE value to build against a different Python version\ncmake -DPython_EXECUTABLE=$MY_PYTHON -DTEST=off --preset linux-debug cpp\npushd cpp\ncmake --build --preset linux-debug\npopd\n</code></pre>"},{"location":"technical/contributing/#4-run-arcticdb","title":"4) Run ArcticDB","text":"<p>Ensure the below is run in the Git project root:</p> <pre><code># PYTHONPATH first part = Python module, second part compiled C++ binary\nPYTHONPATH=`pwd`/python:`pwd`/cpp/out/linux-debug-build/arcticdb/ $MY_PYTHON\n</code></pre> <p>Now, inside the Python shell:</p> <pre><code>from arcticdb import Arctic\n</code></pre> <p>Rather than setting the <code>PYTHONPATH</code> environment variable, you could install the appropriate paths into your Python environment by running (note that this will invoke the build tooling so will compile any changed files since the last compilation):</p> <pre><code>$MY_PYTHON -m pip install -ve .\n</code></pre> <p>Note that as this will copy the binary to your Python installation this will have to be run after each and every change of a C++ file.</p>"},{"location":"technical/contributing/#mamba-and-conda-forge-quickstart","title":"mamba and conda-forge Quickstart","text":"<p>This quickstart uses build dependencies from conda-forge. It is a pre-requisite for releasing ArcticDB on conda-forge.</p> <p>\u26a0\ufe0f At the time of writing, installing ArcticDB with this setup under Windows is not possible since no distribution of folly for Windows is not available on conda-forge. For tracking progress on packaging folly for Windows on conda-forge, see: <code>conda-forge/folly-feedstock#98</code></p> <ul> <li>Install <code>mamba</code></li> <li>Create the <code>arcticdb</code> environment from its specification (<code>environment_unix.yml</code>):</li> </ul> <pre><code>mamba env create -f environment_unix.yml\n</code></pre> <ul> <li>Activate the <code>arcticdb</code> environment (you will need to do this for every new shell session):</li> </ul> <pre><code>mamba activate arcticdb\n</code></pre> <ul> <li>Build and install ArcticDB in the <code>arcticdb</code> environment using dependencies installed in this environement:    We recommend using the <code>editable</code> installation for development:</li> </ul> <pre><code>ARCTICDB_USING_CONDA=1 python -m pip install --verbose --editable .\n</code></pre> <ul> <li>Use ArcticDB from Python:</li> </ul> <pre><code>from arcticdb import Arctic\n</code></pre>"},{"location":"technical/contributing/#faq","title":"FAQ","text":""},{"location":"technical/contributing/#how-do-i-build-against-different-python-versions","title":"How do I build against different Python versions?","text":"<p>Run <code>cmake</code> (configure, not build) with either:</p> <ol> <li>A different version of Python as the first version of Python on your PATH or...</li> <li>Point the <code>Python_EXECUTABLE</code> CMake variable to a different Python binary</li> </ol> <p>Note that to build the ArcticDB C++ tests you must have the Python static library available in your installation!</p>"},{"location":"technical/contributing/#how-do-i-run-the-python-tests","title":"How do I run the Python tests?","text":"<p>See running tests below.</p>"},{"location":"technical/contributing/#how-do-i-run-the-c-tests","title":"How do I run the C++ tests?","text":"<p>See running tests below.</p>"},{"location":"technical/contributing/#how-do-i-specify-how-many-cores-to-build-using","title":"How do I specify how many cores to build using?","text":"<p>This is determined auto-magically by CMake at build time, but can be manually set by passing in <code>--parallel &lt;num cores&gt;</code> into the build command.</p>"},{"location":"technical/contributing/#detailed-build-information","title":"Detailed Build Information","text":""},{"location":"technical/contributing/#docker-image-construction","title":"Docker Image Construction","text":"<p>The above docker image is built from ManyLinux. Build script is located here.</p> <p>GitHub output here.</p> <p>We recommend you use this image for compilation and testing!</p>"},{"location":"technical/contributing/#setting-up-linux","title":"Setting up Linux","text":"<p>The codebase and build system can work with any reasonably recent Linux distribution with at least GCC 8 (10+ recommended) and CMake 3.12 (these instructions assume 3.21+).</p> <p>A development install of Python 3.6+ (with <code>libpython.a</code> or <code>.so</code> and full headers) is also necessary. See pybind11 configuration.</p> <p>We require a Mongo executable for a couple of Python tests on Linux. You can check whether you have it with <code>mongod --version</code>.</p> <p>Search the internet for \"mongo installation Linux\" for instructions for your distro if you do not already have <code>mongod</code> available.</p>"},{"location":"technical/contributing/#dependencies-by-distro","title":"Dependencies by distro","text":"Distro Versions reported to work Packages Ubuntu 20.04, 22.04 build-essential g++-10 libpcre3-dev libsasl2-dev libsodium-dev libkrb5-dev libcurl4-openssl-dev python3-dev Centos 7 devtoolset-10-gcc-c++ openssl-devel cyrus-sasl-devel devtoolset-10-libatomic-devel libcurl-devel python3-devel"},{"location":"technical/contributing/#setting-up-windows","title":"Setting up Windows","text":"<p>We recommend using Visual Studio 2022 (or later) to install the compiler (MSVC v142 or newer) and tools (Windows SDK, CMake, Python).</p> <p>The Python that comes with Visual Studio is sufficient for creating release builds, but for debug builds, you will have to separately download from Python.org.</p>"},{"location":"technical/contributing/#pre-commit-hooks-setup","title":"pre-commit hooks setup","text":"<p>We use pre-commit to run some checks on the codebase before committing.</p> <p>To install the pre-commit hooks, run:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>This will install the pre-commit hooks into your local git repository.</p> <p>If you want to run the pre-commit hooks on all files, run:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"technical/contributing/#running-python-tests","title":"Running Python tests","text":"<p>With <code>python</code> pointing to a Python interpeter with <code>ArcticDB</code> installed/on the <code>PYTHON_PATH</code>:</p> <pre><code>python -m pip install arcticdb[Testing]\npython -m pytest python/tests\n</code></pre>"},{"location":"technical/contributing/#running-c-tests","title":"Running C++ tests","text":"<p>Configure ArcticDB with TEST=on (default):</p> <pre><code>cmake -DPython_EXECUTABLE=&lt;path to python&gt; --preset linux-debug cpp\n</code></pre> <p>Note that <code>&lt;path to python&gt;</code> must point to a Python that is compatible with Development.Embed. This will probably be the result of installing <code>python3-devel</code> from your dependency manager.</p> <p>Inside the provided docker image, <code>python3-devel</code> resolves to Python 3.6 installed at <code>/usr/bin/python3</code>, so the resulting command will be:</p> <pre><code>cmake -DPython_EXECUTABLE=/usr/bin/python3 -DTEST=ON --preset linux-debug cpp\n</code></pre> <p>Then invoke the CMake build as normal and run the compiled test binary.</p>"},{"location":"technical/contributing/#cibuildwheel","title":"CIBuildWheel","text":"<p>Our source repo works with CIBuildWheel which runs the compilation and tests against all supported Python versions in isolated environments. Please follow their documentation.</p>"},{"location":"technical/contributing/#configurations","title":"Configurations","text":""},{"location":"technical/contributing/#cmake-presets","title":"CMake presets","text":"<p>To make it easier to set and share all the environment variables, config and commands, we recommend using the CMake presets feature.</p> <p>Recent versions of some popular C++ IDEs support reading/importing these presets: * Visual Studio &amp; Code * CLion</p> <p>And it's equally easy to use on the command line (see example).</p> <p>We already ship a <code>CMakePresets.json</code> in the cpp directory, which is used by our builds. You can add a <code>CMakeUserPresets.json</code> in the same directory for local overrides. Inheritance is supported.</p> <p>If you're working on Linux but not using our Docker image, you may want to create a preset with these <code>cacheVariables</code>: * <code>CMAKE_MAKE_PROGRAM</code> - <code>make</code> or <code>ninja</code> should work * <code>CMAKE_C_COMPILER</code> and <code>CMAKE_CXX_COMPILER</code> - If your preferred compiler is not <code>cc</code> and <code>cxx</code></p> <p>More examples:</p> Windows Preset to specify a Python version <pre><code>{\n\"version\": 3,\n\"configurePresets\": [\n{\n\"name\": \"alt-vcpkg-debug:py3.10\",\n\"inherits\": \"windows-cl-debug\",\n\"cacheVariables\": {\n\"Python_ROOT_DIR\": \"C:\\\\Program Files\\\\Python310\"\n},\n\"environment\": {\n\"PATH\": \"C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\Scripts;C:\\\\Program Files\\\\Python310;$penv{PATH}\",\n\"PYTHONPATH\": \"C:\\\\Program Files\\\\Python310\\\\Lib;C:\\\\Users\\\\me\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\"\n}\n}\n],\n\"buildPresets\": [\n{\n\"name\": \"alt-vcpkg-debug:py3.10\",\n\"configurePreset\": \"alt-vcpkg-debug:py3.10\",\n\"inheritConfigureEnvironment\": true\n}\n]\n}\n</code></pre>"},{"location":"technical/contributing/#vcpkg-caching","title":"vcpkg caching","text":"<p>We use vcpkg to manage the C++ dependencies.</p> <p>Compiling the dependencies uses a lot of disk space. Once CMake configuration is done, you can remove the <code>cpp\\vcpkg\\buildtrees</code> folder.</p> <p>You may also want to configure some caches: * Binary caching * Asset caching</p>"},{"location":"technical/contributing/#pybind11-configuration","title":"pybind11 configuration","text":"<p>We augmented pybind11's Python discovery with our own PythonUtils to improve diagnostics. Please pay attention to warning messages from <code>PythonUtils.cmake</code> in the CMake output which highlights any configuration issues with Python.</p> <p>We compile against the first <code>python</code> on the <code>PATH</code> by default.</p> <p>To override that, use one of the following CMake variables*:</p> <ul> <li> <p><code>Python_ROOT_DIR</code> - The common path \"prefix\" for a particular Python install.   Usually, the <code>python</code> executable is in the same directory or the <code>bin</code> subdirectory.   This directory should also contain the <code>include</code> and <code>lib(rary)</code> subdirectories.   E.g. <code>/usr</code> for a system-wide install on *nix;   <code>/opt/pythonXX</code> for a locally-managed Python install;   <code>/home/user/my_virtualenv</code> for a virtual env;   <code>C:\\Program Files\\PythonXX</code> for a Windows Python install</p> </li> <li> <p><code>Python_EXECUTABLE</code> - The path to the Python executable. (CMake 3.15+)   CMake will try to extract the <code>include</code> and <code>library</code> paths by running this program.   This differs from the default behaviour of FindPython.</p> </li> </ul> <p>(* Note CMake variables are set with <code>-D</code> on the CMake command line or with the <code>cacheVariables</code> key in <code>CMake*Presets.json</code>.    The names are case-sensitive.</p> <p>(Only) <code>Python_ROOT_DIR</code> can also be set as an environment variable.    Setting the others in the environment might have no effect.)</p>"},{"location":"technical/migration/","title":"Migrating from Arctic","text":""},{"location":"technical/migration/#arcticdb-vs-arctic","title":"ArcticDB vs Arctic","text":"<p>ArcticDB is API-compatible with Arctic. Please note however that it is not data compatible. You cannot use ArcticDB to read Arctic data, nor use Arctic to write data that ArcticDB can read.</p>"},{"location":"technical/migration/#how-can-i-migrate-from-arctic-to-arcticdb","title":"How can I migrate from Arctic to ArcticDB","text":"<p>There are two ways that you can migrate data from Arctic to ArcticDB.</p> <ul> <li>Manual migration: The easiest way to migrate relatively small datasets is to read the data out of Arctic, and then write it using ArcticDB. As the ArcticDB is API compatible, this should be relatively simple.</li> <li>Data Conversion: Large data estates can be converted from Arctic to ArcticDB by using automated tooling that we can provide. For more information, please contact us via our website or email us at arcticdb@man.com. </li> </ul>"},{"location":"technical/on_disk_storage/","title":"ArcticDB On-Disk Storage Format","text":"<p>ArcticDB uses a custom storage format that differs from Parquet, HDF5 or other similar well-known columnar storage formats. This page provides a high-level description of this format.</p> <p>The ArcticDB storage engine is designed to work with any key-value storage backend and currently has full optimised support for the following backends:</p> <ol> <li>S3</li> <li>LMDB</li> </ol> <p>The structure of the stored data is optimised for that storage - for example, when using S3 all related paths (or keys) share a common prefix to optimise for <code>ListObjects</code> calls.</p> <p>Despite this storage specific optimisation, the hierarchy and segment format remain identical regardless of the backend storage. These components are described below.</p>"},{"location":"technical/on_disk_storage/#structural-overview","title":"Structural overview","text":"<p>The above diagram provides an overview of this format, illustrating how ArcticDB manages the symbol metadata, version history and index layout of the data it stores. Not shown is the binary format used to store ArcticDB segments.</p> <p>Note</p> <p>Please note that the key formatting illustrated on the left hand side of the above diagram is specific to S3. The formatting may differ depending on the underlying storage and as such the key paths might not match  when using another storage engine such as LMDB.</p> <p>The ArcticDB storage format is comprised of 4 layers; the Reference Layer, the Version Layer, the Index Layer and finally, the Data Layer. For a full definition of a key, please see the above diagram. </p>"},{"location":"technical/on_disk_storage/#reference-layer","title":"Reference Layer","text":"<p>The reference layer maintains an active pointer to the head of the version layer linked list, which enables fast retrieval of the latest version of a symbol. This pointer is stored in the Data Segment as illustrated in the above diagram. As a result, the reference layer is the only mutable part of ArcticDB's storage format with the value of each reference-layer-key able to be overwritten (hence using a Reference Key, rather than an Atom Key)</p> <p>The reference layer primarily stores keys of type Version Reference, as documented below.</p>"},{"location":"technical/on_disk_storage/#version-layer","title":"Version Layer","text":"<p>The version layer contains a linked list of immutable atom keys/values. Each element of the linked list contains two pointers in the data segment; one points to the next entry in the linked list, and the other points to an index key, providing a route through to the index layer of the storage structure. As a result, traversing the version layer linked list for a symbol allows you to travel backwards through time to retrieve data as it were at a previous version/point in time. The version layer also contains information about which versions have been deleted, and which are still \"live\".</p> <p>This means that for symbols with a lot of versions, this linked list can get quite long, and so reading old versions (using the <code>as_of</code> argument) can become slower as lots of tiny object reads are required in order to find the relevant index key. A method will soon be added to the <code>Library</code> API allowing users to \"compact\" this linked list into a few larger objects.</p> <p>The version layer primarily stores keys of type Version, as documented below.</p>"},{"location":"technical/on_disk_storage/#index-layer","title":"Index Layer","text":"<p>The index layer is an immutable layer that provides a B-Tree index over the data layer. Much like the reference and version layer, this utilises data segments containing data pointers. Each pointer is simply a key that that contains a data segment. </p> <p>For more information on the data stored in this layer, see the Structural Overview diagram.</p> <p>The index layer primarily stores keys of type Table Index, as documented below.</p>"},{"location":"technical/on_disk_storage/#data-layer","title":"Data Layer","text":"<p>The data layer is an immutable layer that contains compressed data segments. Dataframes provided by the user are sliced by both columns and rows, in order to facilitate rapid date-range and column searching during read operations. See the documentation for the <code>rows_per_segment</code> and <code>columns_per_segment</code> library configuration options for more details.</p> <p>The data layer primarily stores keys of type Table Data, as documented below.</p>"},{"location":"technical/on_disk_storage/#arcticdb-key-types","title":"ArcticDB Key Types","text":""},{"location":"technical/on_disk_storage/#what-is-an-arcticdb-key","title":"What is an ArcticDB key?","text":"<p>ArcticDB stores data in either LMDB or object storage (S3). Regardless of the backend though, ArcticDB stores data in well-defined key types where a key type defines the purpose of the associated value and the structure of the name/path of the key. Examples of how the paths differ for S3 are shown below.</p> <p>Note that where this documentation refers to a Key Type key (for example a version reference key), it refers to both the S3/LMDB key and the associated S3/LMDB value.</p>"},{"location":"technical/on_disk_storage/#key-types","title":"Key Types","text":"Key Type Atom/Reference S3 Prefix Purpose Version Reference Reference vref Maintains the top level pointer to the latest version of a symbol Version Atom ver Maintains pointers to index keys for one or more versions of a symbol Table Index Atom tindex Maintains an index structure over the data Table Data Atom tdata Maintains the data for a table Symbol List Atom sl Caches symbol addition/removals <p>Note that Atom keys are immutable. Reference keys are not immutable and therefore the associated value can be updated. </p> <p>Version Ref</p> <p>In this documentation, <code>Version Reference key</code> is sometimes shortened to <code>Version Ref key</code>. </p>"},{"location":"technical/on_disk_storage/#example-paths","title":"Example paths","text":"<p>The paths below are examples of S3-based paths. LMDB ArcticDB libraries may have slightly different paths. </p> Key Type Example Path Version Ref {bucket}/{library prefix}/{library storage id}/vref/sUtMYSYMBOL Symbol List {bucket}/{library prefix}/{library storage id}/*sSt*__add__*6*1632469542244313925*8986477046003934300*ORJDSG8GU4_6*ORJDSG8GU4_6"},{"location":"technical/on_disk_storage/#additional-information","title":"Additional Information","text":""},{"location":"technical/on_disk_storage/#data-layer-fragmentation","title":"Data Layer Fragmentation","text":"<p>As the example given in the Structural Overview diagram demonstrates, Atom Keys in the data layer from older versions are re-used (i.e. pointed to) by the new version's index layer if the data in them is still needed. This is always trivially true for <code>append</code> operations, and this is conceptually simpler, so the example we will work with here is based on <code>append</code>. It is worth noting that all of the same arguments also apply to <code>update</code> as well though.</p> <p>By re-using data layer keys from the previous version, calls to <code>append</code> are as efficient as possible, as they do not need to read any of the data layer keys from the previous version. However, this can result in the data layer becoming fragmented, in the sense that data in the data layer is spread over a lot of small objects in storage, and this can have a negative performance impact on <code>read</code> operations.</p> <p>Consider a use case where a symbol with 10 columns, all of 8 byte numeric types, is appended to once per minute. This means that each day, 1,440 data layer segments are written, each with just 80 bytes of information in. Calling <code>read</code> for a days worth of data therefore requires 1,440 reads of these tiny data layer objects, which will be much less efficient than reading a single object of 115,200 bytes.</p> <p>We will soon be adding an API to perform exactly this operation, re-slicing data such that subsequent reads can be as efficient as possible, without harming the efficiency of existing <code>append</code> or <code>update</code> operations.</p>"},{"location":"technical/on_disk_storage/#symbol-list-caching","title":"Symbol List Caching","text":"<p>Speeding up listing symbols</p> <p>The below documentation details the architecture for the symbol list cache.</p> <p>It explains that to speed up <code>list_symbols</code>, simply run <code>list_symbols</code> through to completion frequently.  The cache is built on first run and compacted afterwards.  This will speed up <code>list_symbols</code> for all accessors of the library - not just the user that runs <code>list_symbols</code>. </p> <p><code>list_symbols</code> is a common operation to perform on an ArcticDB library. As this returns a list of \"live\" symbols (those for which at least one version has not been deleted), using the data structures described above, this involves:</p> <ul> <li>Loading a list of version reference keys in the library.</li> <li>For each version reference key:<ul> <li>Traverse the version key linked-list until either a live version is found, or it is established that all versions have been deleted.</li> </ul> </li> </ul> <p>For many libraries, these operations will be quick enough. However, if there are millions of symbols, or if the first live version to be found for symbols is very far back in the version key linked-list, then this method can be prohibitively expensive.</p> <p>Therefore, ArcticDB maintains a cache of live symbols, such that the <code>list_symbols</code> call should always return quickly. This works by writing special atom keys into the storage that track when symbols are created or deleted, and the time that the operation happened. These Symbol List atom keys are of the form:</p> <ul> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*&lt;timestamp 1&gt;*&lt;content hash&gt;*&lt;symbol name&gt;*&lt;symbol name&gt;</code> - signifies that <code>&lt;symbol name&gt;</code> was created at <code>&lt;timestamp 1&gt;</code></li> <li><code>&lt;library prefix&gt;/sl/*sSt*__delete__*0*&lt;timestamp 2&gt;*&lt;content hash&gt;*&lt;symbol name&gt;*&lt;symbol name&gt;</code> - signifies that <code>&lt;symbol name&gt;</code> was deleted at <code>&lt;timestamp 2&gt;</code></li> </ul> <p>The operation to list symbols then involves:</p> <ul> <li>Read all of the symbol list atom keys for the library (this resolves to a <code>ListObjects</code> call with S3 storage).</li> <li>Separate the keys by the symbol they refer to being created/deleted.</li> <li>For each symbol:<ul> <li>Check if the most recent operation was a creation or a deletion.</li> </ul> </li> </ul> <p>Symbols for which the latest operation is a creation are then returned to the caller of <code>list_symbols</code>.</p> <p>Without any housekeeping, this process could lead to unbounded growth in the number of symbol list atom keys in the library. Even worse, many of these keys would contain redundant information, as we only care about the latest operation for each symbol. Therefore, whenever <code>list_symbols</code> is called by a client with write permissions on the library, if there are too many (500 by default, see the Runtime Configuration page for details on how to configure) symbol list atom keys, all of the information from these keys is compacted into a single symbol list atom key, and the old keys are deleted. For example, if there were 4 symbol list atom keys:</p> <ul> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t0*&lt;content hash&gt;*symbol1*symbol1</code> Create \"symbol1\" at t0</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__delete__*0*t1*&lt;content hash&gt;*symbol1*symbol1</code> Delete \"symbol1\" at t1</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t2*&lt;content hash&gt;*symbol12*symbol2</code> Create \"symbol2\" at t2</li> <li><code>&lt;library prefix&gt;/sl/*sSt*__add__*0*t3*&lt;content hash&gt;*symbol1*symbol1</code> Create \"symbol1\" at t3</li> </ul> <p>They would be compacted into a single object stating that \"symbol1\" and \"symbol2\" were both alive at time t3. The key for this object is of the form <code>&lt;library prefix&gt;/sl/*sSt*__symbols__*0*t3*&lt;content hash&gt;*0*0</code>.</p> <p>Astute observers will correctly take issue with basing logical decisions on timestamps in a purely client-side database with no synchronisation between clocks of different clients enforced. As such, this cache can diverge from reality, if two different clients create and delete the same symbol at about the same time, and this is the most likely cause of odd behaviour such as <code>lib.read(symbol)</code> working, but <code>symbol in lib.list_symbols()</code> returning <code>False</code>. If this happens, <code>lib.reload_symbol_list()</code> should resolve the issue.</p>"},{"location":"technical/releasing/","title":"Release Tooling","text":"<p>This document details the release process for ArcticDB.  ArcticDB is released onto PyPi and conda-forge.</p>"},{"location":"technical/releasing/#1-create-a-new-tag","title":"1. Create a new tag","text":"<p>Navigate to the Tag Release Github Action.</p> <p>Click <code>Run Workflow</code> on the right hand side: 1. Type in the new version number 2. Click <code>Run workflow</code>.</p> <p>Leave <code>Bump branch to the next version</code> as <code>No</code>.  This will create a branch off of <code>master</code> incrementing the version in <code>setup.cfg</code> but,  as of the time of writing, we are leaving that unchanged.</p>"},{"location":"technical/releasing/#2-update-the-version-number-in-the-tag","title":"2. Update the version number in the tag","text":"<p>This should not be a manual process - but is required for now!</p> <p>We need to update the version for  <code>arcticdb.__version__</code>.</p> <p>To do this, force push a new commit to the tag:</p> <pre><code>git fetch --all --tags\ngit checkout &lt;NEWLY CREATED TAG&gt;\nvi python/arcticdb/__init__.py\ngit add python/arcticdb/__init__.py\ngit commit -m 'Manually update version'\ngit tag &lt;NEWLY CREATED TAG&gt; -f\ngit push origin &lt;NEWLY CREATED TAG&gt; -f\n</code></pre> <p>The result should look similar to this commit.</p> <p>The build will now be running for the tag.</p>"},{"location":"technical/releasing/#3-update-conda-forge-recipe","title":"3. Update conda-forge recipe","text":"<p><code>regro-cf-autotick-bot</code> generally opens a PR on ArcticDB's feedstock for each new release of ArcticDB upstream.</p> <p>You can update such a PR or create a new one to release a version, updating the conda recipe. Here's an example.</p> <p>You will need to update:</p> <ol> <li>The version, pointing to the tag created in step 1. </li> <li>The <code>sha256sum</code> of the source tarball</li> <li>The build number (i.e. <code>number</code> under the <code>build</code> section) to 0</li> <li>Dependencies (if they have changed since the last version)</li> <li>Rerender the feedstock's recipe to create Azure CI jobs' specification for all variants of the package</li> </ol> <p>A PR is generally open with a todo-list summarizing all the required steps to perform, before an update to the feedstock.</p>"},{"location":"technical/releasing/#4-release-to-pypi","title":"4. Release to PyPi","text":"<p>After building, GitHub Actions job you kicked off in step 2 after comitting the tag will be waiting on approval to deploy to PyPi.  Find the job and click approve to deploy.</p>"},{"location":"technical/releasing/#5-release-to-conda","title":"5. Release to Conda","text":"<p>Merge the PR created in step 3. </p> <p>It will build packages, pushing them to the <code>cf-staging</code> channel before publishing them on the <code>conda-forge</code> channel for validation.</p> <p>Packages are generally available a few dozen minutes after the CI runs' completion on <code>main</code>.</p>"},{"location":"tutorials/fundamentals/","title":"ArcticDB Fundamentals","text":"<p>This tutorial will walk through the fundamentals of ArcticDB:</p> <ol> <li>Accessing libraries</li> <li>Writing data</li> <li>Reading data</li> <li>Modifying data</li> </ol> <p>To start, let's import Arctic:</p> <pre><code>from arcticdb import Arctic\n</code></pre>"},{"location":"tutorials/fundamentals/#accessing-libraries","title":"Accessing libraries","text":"<p>Connect to your storage:</p> <pre><code># Connect using defined keys\nac = Arctic('s3s://s3.eu-west-2.amazonaws.com:arctic-test-aws?access=&lt;access key&gt;&amp;secret=&lt;secret key&gt;')\n# Leave AWS SDK to work out auth details \nac = Arctic('s3s://s3.eu-west-2.amazonaws.com:arctic-test-aws?aws_auth=true)\n</code></pre> <p>For more information on how the AWS SDK configures authentication without utilising defined keys, please see the AWS documentation.</p> <p>Access a library using either the <code>[library_name]</code> notation or the <code>get_library</code> method:</p> <pre><code>lib = ac['library']\n# ...equivalent to...\nlib = ac.get_library['library']\n</code></pre> <p>Let's see what data is already present:</p> <pre><code>&gt;&gt;&gt; lib.list_symbols()\n['sym_2', 'sym_1', 'symbol']\n</code></pre>"},{"location":"tutorials/fundamentals/#arcticdb-api","title":"ArcticDB API","text":"<p>ArcticDB's API is built around four main primitives that each operate over a single symbol.</p> <ol> <li>write: Creates a new version consisting solely of the item passed in.</li> <li>append: Creates a new version consisting of the item appended to the previously-written data.</li> <li>update: Creates a new version consisting the previous data patched with the provided item.</li> <li>read: Retrieves the given version (if no version is provided the latest version is used).</li> </ol> <p>These primitives aren't exhaustive, but cover most use cases. We'll show the usage of these primitives.</p>"},{"location":"tutorials/fundamentals/#writing-data","title":"Writing data","text":"<p>Let's start by generating some data. The below snippet generates some random data with a datetime index:</p> <pre><code>import numpy as np\nimport pandas as pd\nNUM_COLUMNS=10\nNUM_ROWS=100_000\ndf = pd.DataFrame(np.random.randint(0,100,size=(NUM_ROWS, NUM_COLUMNS)), columns=[f\"COL_{i}\" for i in range(NUM_COLUMNS)], index=pd.date_range('2000', periods=NUM_ROWS, freq='h'))\n</code></pre> <p>Let's take this data and write it to ArcticDB:</p> <pre><code>&gt;&gt;&gt; lib.write(\"my_data\", df)\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'NoneType'&gt;,version=0,metadata=None,host=local)\n</code></pre>"},{"location":"tutorials/fundamentals/#reading-data","title":"Reading data","text":"<p>To read the data, simply use the <code>read</code> primitive:</p> <pre><code>&gt;&gt;&gt; data = lib.read(\"my_data\")\n&gt;&gt;&gt; data\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'pandas.core.frame.DataFrame'&gt;,version=0,metadata=None,host=local)\n</code></pre> <p>Note that you get back a <code>VersionedItem</code> - it allows us to retrieve the version of the written data:</p> <pre><code>&gt;&gt;&gt; data.version\n0\n</code></pre> <p>As this is the first write to this symbol, the version is <code>0</code>. To retrieve the data:</p> <pre><code>&gt;&gt;&gt; data.data\n</code></pre>"},{"location":"tutorials/fundamentals/#slicing-and-filtering","title":"Slicing and filtering","text":"<p>See the getting started guide for more information.</p>"},{"location":"tutorials/fundamentals/#modifying-data","title":"Modifying data","text":"<p>Let's append some data. First, note that the data we've written ends in 2011:</p> <pre><code>&gt;&gt;&gt; data.data.tail()\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  COL_8  COL_9\n2011-05-29 11:00:00     44     94     70     32     91      4     35     19     74     53\n2011-05-29 12:00:00     79     51     67     48      8     83     46     54     86     38\n2011-05-29 13:00:00     60     98     74      4     81     86     64     78     13     32\n2011-05-29 14:00:00     27     24     16      6     84     99     11     94     29      4\n2011-05-29 15:00:00     81     76     52     93     31     91     64      2     26     78\n</code></pre> <p>That's simply because the data we generated started on January 1st, 2000 at 00:00 at consists of 100,000 rows, incrementing one hour at a time. When <code>append</code>-ing data, the data you are appending must begin  after the existing data ends. As a result, let's generate some data that begins in 2012:</p> <pre><code>df_to_append = pd.DataFrame(np.random.randint(0,100,size=(NUM_ROWS, NUM_COLUMNS)), columns=[f\"COL_{i}\" for i in range(NUM_COLUMNS)], index=pd.date_range('2012', periods=NUM_ROWS, freq='h'))\n</code></pre> <p>Now let's append!</p> <pre><code>&gt;&gt;&gt; lib.append(\"my_data\", df_to_append)\nVersionedItem(symbol=my_data,library=test_fundamentals_1,data=&lt;class 'NoneType'&gt;,version=1,metadata=None,host=local)\n</code></pre> <p><code>append</code> has created a new version of the data. When reading version 0, the data will end in 2011. When reading version 1, the data will end in 2023.</p>"},{"location":"tutorials/fundamentals/#update","title":"Update","text":"<p>If append can only append date that begins after the existing data ends, then it begs the question - how do we mutate data?</p> <p>The answer is that we use the <code>update</code> primitive. <code>update</code> overwrites (creating a new version - nothing is lost!) existing symbol data with the data that is passed in.  Note that the entire range between the first and last index entry in the existing data is replaced in its entirety with the data that is passed in, adding additional index entries if required. This means <code>update</code> is a contiguous operation - see the documentation of <code>update</code> for more information.</p>"},{"location":"tutorials/fundamentals/#time-travel","title":"Time travel!","text":"<p>ArcticDB is bitemporal - all new versions are timestamped! Let's pull in the first version of the data, prior to the <code>append</code>:</p> <pre><code>&gt;&gt;&gt; lib.read(\"my_date\", as_of=0).data.tail()\n                     COL_0  COL_1  COL_2  COL_3  COL_4  COL_5  COL_6  COL_7  COL_8  COL_9\n2011-05-29 11:00:00     44     94     70     32     91      4     35     19     74     53\n2011-05-29 12:00:00     79     51     67     48      8     83     46     54     86     38\n2011-05-29 13:00:00     60     98     74      4     81     86     64     78     13     32\n2011-05-29 14:00:00     27     24     16      6     84     99     11     94     29      4\n2011-05-29 15:00:00     81     76     52     93     31     91     64      2     26     78\n</code></pre> <p>Note that it ends in 2011 - it's like the <code>append</code> never happened. <code>as_of</code> can take a timestamp (<code>datatime.datetime</code> or <code>pd.Timestamp</code>) as well.</p>"},{"location":"tutorials/metadata/","title":"Metadata","text":"<p>ArcticDB enables you to store arbitrary binary-blobs alongside symbols and versions. The data is pickled when using the Python API. Note that there is a 4GB limit to the size of a single blob. </p> <p>The below example shows a basic example of writing and reading metadata (in this case a pickled Python dictionary):</p> <pre><code># This example assumes the below variables (host, bucket, access, secret) are validly set\nac = Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\nlibrary = \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nmetadata = {\n    \"This\": \"is\",\n    \"a\": \"Python\",\n    \"Dictionary\": \"!\"\n}\n\nlib.write(\"meta\", data=pd.DataFrame(), metadata=metadata)  # or write_metadata can be used - will still create a new version, but doesn't require `data` to be passed in\n\nassert lib.read(\"meta\").metadata == metadata\nassert lib.read_metadata(\"meta\").metadata == metadata  # Same as read, but doesn't return data from storage\n</code></pre>"},{"location":"tutorials/metadata/#practical-example-using-metadata-to-track-vendor-timelines","title":"Practical example - using metadata to track vendor timelines","text":"<p>One common example for metadata is to store the vendor-provided date alongside the version. For example, let's say we are processing three files - <code>data-2004-01-01.csv</code>, <code>data-2004-01-02.csv</code> and <code>data-2004-01-03.csv</code>. Each file name contains a date which we'd like to be able to store along side the version information in ArcticDB.</p> <p>We can do this using the following code:</p> <pre><code>import glob\nimport datetime\n\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\nlibrary = \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nfile_names = glob.glob('*.csv')  # returns ['data-2004-01-01.csv', 'data-2004-01-02.csv', 'data-2004-01-03.csv']\n\nfor i, name in enumerate(file_names):\n    data = pd.read_csv('name')\n    date = datetime.datetime.strptime(name[5:][:-4], '%Y-%m-%d')\n\n    if i == 0:\n        lib.write(\"symbol\", data, metadata={\"vendor_date\": date})\n    else:\n        lib.append(\"symbol\", data, metadata={\"vendor_date\": date})\n</code></pre> <p>We'll now use this to read the data along the vendor-provided timeline - that is, we'll retrieve the data as if we had written each file on the day it was generated. We'll read all metadata entries for all versions of the symbol and select the date that is most recent with respect to a given date (in this case 2004-01-02):</p> <pre><code># Continuing from the previous code snippet\nmetadata = [\n    (v[\"version\"], lib.read_metadata(\"symbol\", as_of=v[\"version\"]).metadata.get(\"vendor_date\"))\n    for v in lib.list_versions(\"symbol\")\n]\nsorted_metadata = sorted(metadata, key=lambda x: x[1])\n\nversion_to_read_index = bisect_right([x[1] for x in sorted_metadata], datetime.datetime(2004, 1, 2))\nlib.read(\"symbol\", as_of=sorted_metadata[version_to_read_index - 1][0])\n</code></pre> <p>Note that if the data is written across multiple symbols, then ArcticDB Snapshots can be used to achieve the same result. </p>"},{"location":"tutorials/parallel_writes/","title":"Parallel Writes","text":"<p>As mentioned, ArcticDB fundamentally does not support concurrent writers to a single symbol - unless the data is concurrently written as staged data!</p> <p>Staged data is not available to read, and requires a process to finalize all staged data prior to it being available for reading. Each unit of staged data must not overlap with any other unit of staged data -  as a result staged data must be timeseries indexed. The below code uses Spark to concurrently write to a single symbol in parallel, before finalizing the data:</p> <pre><code>import pyspark\n\n# This example assumes the below variables (host, bucket, access, secret) are validly set\nac = Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\n\ndef _load(work):\n    # This method is run in parallel via Spark.\n    host, bucket, access, secret, symbol, library, file_path = work\n    ac = Arctic(f\"s3://{host}:{bucket}?access={access}&amp;secret={secret}\")\n\n    library = ac[library]\n\n    df = pd.read_csv(file_path)\n    df = df.set_index(df.columns[0])\n    df.index = df.index.to_datetime()\n\n    # When staged, the written data is not available to read until finalized.\n    library.write(symbol, df, staged=True)\n\nsymbol = \"my_data\"\nlibrary = \"my_library\"\n\nconf = SparkConf().setAppName('appName').setMaster('local')\nsc = SparkContext(conf=conf)\n\n# Assumes there are a set of CSV files in the current directory to load from\ndata = [(host, bucket, access, secret, symbol, library, f) for f in glob.glob(\"*.csv\")]\ndist_data = sc.parallelize(data)\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\nret = dist_data.map(_load)\nret.collect()\n\nlibrary.finalize_staged_data(symbol)\n\ndata = library.read(symbol)\n</code></pre>"},{"location":"tutorials/snapshots/","title":"Snapshots","text":"<p>ArcticDB enables multi-symbol snapshotting. Snapshots enable multiple symbols to be versioned together via a human readable string name. </p> <p>In practise this is useful to tie derived data with the source data.</p> <pre><code># This example assumes the below variables (host, bucket, access, secret) are validly set\nac = Arctic(f\"s3://{HOST}:{BUCKET}?access={ACCESS}&amp;secret={SECRET})\n\nlibrary= \"my_library\"\n\nif library not in ac.list_libraries():\n    ac.create_library(library)\n\nlibrary = ac[library]\n\n# Assumes there are CSV files containing pricing data and factor data. Each time we've written ALL new factor files\n# to their symbol, we'll take a snapshot across all symbols.\nfor i, f in enumerate(sorted(glob.glob('*.csv'), key=lambda f: f.split('_')[1].split('.')[0])):\n    df = pd.read_csv(f)\n    if 'FACTORS' in f:\n        library.write(f, df)\n        # SNAP_{i} will forever point to all symbols that exist at this time at their current latest version\n        library.snapshot(f\"SNAP_{i}\")\n    else:\n        df = df.set_index(df.columns[0])\n        df.index = df.index.to_datetime()\n\n        library.append(pricing_symbol, df, write_if_missing=True)\n\nsnapshots = library.list_snapshots()\nsymbols = library.list_symbols(snapshot_name=list(snapshots.keys())[0])\n</code></pre> <p>To generate this data, the following code can be used:</p> <pre><code>import argparse\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\ndef run(num_files, num_symbols):\n    starting_date = datetime.today() - timedelta(weeks=num_files)\n    starting_date = datetime(starting_date.year, starting_date.month, starting_date.day)\n\n    for file_num in range(num_files):\n        index_size = 7 * 24\n        this_file_starting_date = starting_date + timedelta(weeks=file_num)\n\n        df = pd.DataFrame(np.random.randint(0,index_size,size=(index_size, num_symbols)), columns=['SYM_%d' % i for i in range(num_symbols)])\n\n        df.index = pd.date_range(this_file_starting_date, periods=index_size, freq=\"H\")\n\n        df.to_csv(f\"PRICING_{this_file_starting_date}.csv\")\n        if file_num % 3 == 0:\n            df = pd.DataFrame(np.random.randint(0, 5,size=(5, num_symbols)), columns=['SYM_%d' % i for i in range(num_symbols)])\n            df['FACTORS'] = ['FACTOR_1', 'FACTOR_2', 'FACTOR_3', 'FACTOR_4', 'FACTOR_5']\n            df.to_csv(f\"FACTORS_{this_file_starting_date}.csv\")\n\n        print(f\"Written {file_num + 1} / {num_files}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--num-files', type=int, default=15)\n    parser.add_argument('--symbols-per-file', type=int, default=500)\n\n    args = parser.parse_args()\n\n    run(args.num_files, args.symbols_per_file)\n</code></pre>"}]}