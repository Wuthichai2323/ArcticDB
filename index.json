{"project": "ArcticDB", "project_url": "http://arcticdb.io/", "show_commit_url": "https://github.com/man-group/ArcticDB/commit/", "hash_length": 8, "revision_to_hash": {"42": "8aca4e9cc678650de36a12d31d2e7f3c4cc9a03b", "318": "b70029d5724883dc903fc6a9f95b17de33dde93d", "457": "03b8e00539fc031dcabd3b2ec3accfbc9e8e3587", "480": "bf01831fd860837eec04f0437ee7ee54aca60d00", "637": "c90a21a611b5c6ec2ef4b049981ac5c2ccb8ad08", "727": "4daa3de40dd67ebde39d48797ce05240dba1e05a", "735": "14209d33c87c3325c5a77879a3cfa036894fea52", "737": "95806216ea1bdfbbc0930453adc1db42d3a4e624", "739": "525e6c28ae67faa1fe4ae2bd39d287a75fd7b083", "811": "a1b39e939f509b90a6b4f58c0c846cdfd22ae8fc", "884": "7f92cbca1bab219847ea7c9b6ccfced6b3de3565", "912": "357ead11b55196b3524196164733d2b072e09449", "927": "cf1a7326d8d648f75ec0daa3044fef4dfa71c7c5", "942": "bdf81500fef77a27f59d0541a8405ff3dfe44029", "970": "d7b6e1b40613f6b4b559a6777d1ac13e63e280bc", "1028": "0a87f5c08de754cfcecf8261ad2c74119069ae65", "1103": "884eedc27666ab6b208a5bb7b4cc9bdb492ff58e", "1194": "3c98e789888d71133a5cccb1ad6d27a0d766f316", "1206": "2f25723f0570eacc63e03fc09de6e9106b61dc4a", "1224": "0f3dd263209c35022a0eac43583e11c7c38742fc", "1225": "1bb8fa063f725ad00282ff18eaea3754c8b085da", "1242": "09aaf7b32b5f5750e1515e7ffaea3779d5e051dc", "1278": "7a449cddb6e7a2164ea03912a934d423a3925455", "1300": "beab8372e6b276a57f4f6d6384b1e7bd8dc59704", "1312": "7baa141aa4a73dea538d6b257dda2f334c85bb46", "1336": "a54daaa294d571a2adbca6acf96edd3c3b550d96", "1354": "1e1d0d70542c2df9f5dc844c037b5136f77771dd", "1369": "0df1889b2e3cef53c00ccc10155186a31341e0ce", "1435": "5f61b27cd7898f376418e2ff7b4e0fcc61e712fc", "1509": "d4fb4834d5645785a2999381572456e2e1a0e2bc", "1518": "bccff006ed94d8a7530367daf67487a2ae2d471a", "1522": "7203c11b2dc111c6f5bc67d42ce66d48fdbafa05", "1537": "1e5b739d56edf61af3bb4d572706262432f72633", "1538": "4f5a6db83710619cb98645cca8b375778c3b7872", "1661": "91994f71840b1b2637d5dd0a3ea16d13d3640dfb", "1704": "8524cbdd76c99d5d4c580f0abc31fa52c9bcc922", "1713": "7b5853c9f38428ae84ce65c6703fd9eb03d7979d", "1890": "8c50cab75bb0a2360dfdbfe454ecda195a3464b3", "1914": "5a38563045eb98941710984f3a8a39583f23cf2f", "2108": "e5c626b209d61639d21a158b0d5cde1ae9b9a31c", "2130": "ada51233bccdab968692e952d40f1c62323cd713", "2255": "1a90cc53c775fe993b756fcdc8648ae643051780", "2304": "545807a2413a511c4f6ef64f7c679fc32f28e650", "2310": "f60ef8fb69e5c3e2939a0ee5d1f55d9017649a7f", "2355": "91b1d563714b2f3c262ceb17e4f3bd8646415a33", "2356": "05fc1a7f13e8c3cf21ff671f7a942687f88c5b42", "2404": "6b72e8652824b991eceb96aea67b2ca4ee68de3b", "2431": "b36736c7d0e6a6c06b809e755964db5ca138eeef", "2507": "7d8c24e75d6897287ba6b97a2c059af026402941", "2509": "5fab651aaee4dd50e7cf97fe9c3f85bf618c3001", "2537": "66171bedffdd5968432e3a799c506b011a34debe", "2641": "052b8e3c61e64448605171e78dccb9b6da9f1adb", "2729": "de6e3cb6f3856edaf55d5ce2ecab8dc01bb05b6c", "2738": "19d67f36b3c4636846c2825aa07d5c92b4796c99", "2759": "03fb62206e9404360616552a41b1934dfd2eeef5", "2763": "efeb3c277af32e0b3f09a481b3fb360faaed2b6a", "2791": "a8664a622b8f4c1f6aa9cacf2742145da1907d00", "2883": "724ecccd67548b5bbd4acd75bf46b317055448d6", "2888": "c14082dc9b021a0fa48185505a8a2640f98c1910", "2889": "c696f598cb7c9e0ae77227007a86e1868be274ac", "2892": "ad257eb14e59f533c4588723b7c89e2856f02c3f", "2898": "b8cf5c7f45c9c1f8aeacfb9705a9acba77ee247a", "2900": "1c5a41918fd650471750062295c9369e1679ebd4", "2907": "89fc8bb90dcc29fed6e39ca3ddd5b9824c64b2b2", "2912": "9a4fadce8190395c4a873037babce412ced6f987", "2918": "8240ab2220494344868d6a1031b58e3679c16e9b", "2925": "47967de95128ad6f69a994bfbf82c1f156319f5f", "2926": "b2c820c700eeb48514c76fd329995217c17118f9", "2935": "cd632fc84df936930989dc3e77ca541927cd8d21", "2944": "13b807c06ab950134a861efc8f8fdfa46076a6db", "2947": "65cb44d99e29a1dcedad76fbcda6af0e6ce6c7eb", "2956": "72f3730d81b3d1d86fd98832c825936f4a290e5b", "2958": "ac846a89dc11130c590798c4d8fc1ae690a37783", "2961": "5af60035e3eb9b74949ba1791e6e83c8d2cc6699", "2974": "e0aa4eaa54ab27907594bf4c6038f64bebf37509", "2976": "efd869096fb69d07ba6bd5635b3e64e799283c5c", "2978": "bf1d7885241e145b7e2c95b0ac49d107f4c873d3", "2981": "0bf5d1b74067ef4fa870f3d7665ec6d792b99d06", "3009": "356972d3c4e78efb84602d3f764dc58bf77d5bbf", "3016": "911a55c5543626d7c0da39e6db2e7d39d716cf60", "3027": "ff4309d06fe2fa506d574264dfccfcb9e7306a69", "3033": "7205a86a8ce0ef45066d78f0357f5e08031a157b", "3039": "9e3a47dd0efd27e37a4047142612b17da4cd9ddc", "3050": "03ff0dc0cde52481a3f2ab2243c35d3c26813322", "3057": "67c78fbfa02e3648ba52c0b76b83659a64696d01", "3058": "b367e7e9b4bfcb1c848a1db92352e8de2fd91bd9", "3069": "1cd7d33bf88f7148ae0aad8f525d5f0f4392147c", "3071": "483019a95f2d841b99ddf9cdb793dfa21398151f", "3085": "841bf40d284b0e5064eae47102d1800de2a446fe", "3088": "c81f8ad2b8f0831cfa855e6b3dc954b180c4c941", "3092": "9b2cbb9f1bf58b70f09370cd74de691083f8e6f6", "3104": "f46592d05942605da6336f8dbf9cf671d5a1d2d4", "3112": "8cf8279b45649f03582202e169aeb6a0f2978732", "3115": "cf391165aed6a3b89b61338c5b4c7f4e0ce92b44", "3118": "31853f18bd5b4ec291515860f9f572d01d6e67b6", "3124": "66a5d19c4a2c3b2423b52a68f984ca8d9b52afce", "3131": "dc6764fb5ef2d159f9f2e35bcf9c68493d94b140", "3135": "cb607222a64cc53012c65ebfbdbbb98090c1125b", "3140": "d404cdcbcd181dc021a162ed589373f44ee74bd1", "3148": "dfeb11207051437388e4574ab2b9636893415150", "3150": "083e0f602910ad86524d719cab1ae367eb06a5a5", "3171": "7bf8b4aa087ed4226d99111ceab18095da1d358c", "3182": "d0f7487f162fcc22239ecd84ae705773cdf9ef51", "3193": "51b8e2a327c9facebe27289946a6d80c38a67982", "3197": "eac6d09dfe67665b1b430569c91f91961987e027", "3203": "a21da2371ca7875f39ff6d14462ed16d0520b9c9", "3228": "b9aabf5942884764f6c641cd9429a5d6f47cd6a1", "3230": "471d89912c91fdae1d2c507f10d5fedd234f9edf", "3231": "8637fa2da367cf39cea88c27ab79a17f9ac308c3", "3236": "86e9f769e3fb555f0037674672c75cd3120132f7", "3239": "bbf594f8c200231bcf50a81c64e396bb42d19cda", "3250": "2881422c59fc8dd0f070daec9d3f495627f68b06", "3262": "6fc2aaa50fff579b178413a7bfd4b5c9c811e214", "3274": "0fc441cf64d46994cdb5b766e68c3d81b60e0086", "3276": "dd4617e309c5b31cebe79816ea43bf1136b59365", "3288": "683d833a675bd18b38417f104b2d0d5e6b4e3d97", "3291": "89817db81728b323ca296ba18a957c8aacfe11bb", "3312": "28feeb340c52664d5f9aec1363f47a499a0bf40e", "3318": "2858895ea4d1b2e2ec1602067c1bf16d88f9a8c7", "3319": "273b43f6b0f6cad1d490d917e2b1a150089c5683", "3327": "ec65b8e5f2d3bd556ecd1e3d73cb23705cafd7e8", "3331": "498c3311dc08f5c8872ccac0ef3dfb278776699c", "3341": "102b1c0ef09db96014a65d6fea007854744e0906", "3345": "030fe5ac1e0a7a94208844c41a3c5e6bfb26fb85", "3366": "83deb987adf6eda8fd1381b3834ddd7e702e051b", "3370": "e0456876ba5c0f749756f6de22c46d0d923f082d", "3376": "6ba2ff168b556044b54a8d6c37ee8e7ff17acf75", "3387": "4d092838343e729ef7579c947b5e8b7dc83bb563", "3399": "7df641fba906a2cba0ce406d4161dff27f6b3946", "3405": "17ef7f0c0b3970227b5fada52e7e2832615ef8d0", "3417": "8461ff17bb6f4768dbed554e52d55afc7b8d681f", "3427": "1bb57d7b02fb2a101831dc9ec1a6faf1dd3160ee", "3433": "bdf44d6a0268c55fcd5d3da619812325e0e8b44f", "3443": "2cad4622122e730032271a0e936a8c590f611cd4", "3456": "a398c24a1f875cc848d016fbfdc0347f33e81d63", "3460": "c6a0567bb3f64537c350cebb23fd97043f6e577e", "3462": "dd072b12b24ba3b3e6d033d169e2bf4e32f925e7", "3463": "56e88c19073613255cf3b606f88de6652af3987f", "3473": "07fcc09413230f136b3903deaa9262ce466bb6de", "3481": "3109deeec996ccc1c4d845c7fea9d7ab9edf2746", "3487": "52cdc81b7f175ad410b4a3626b366e3ab1aec3a1", "3489": "afaba9ad4ebdf93e8b9a9d814eba3309f34dd830", "3491": "392c9abbfee136203f31fcd4e82db59ea72516a2", "3495": "008fa54cfe123c9ac4276643ebb5e22126d7a3c0", "3503": "dd7894f2f55c13293b699826aeaf603d0793a793", "3507": "b157c924171e5bfdd4e0a2658c611917037bcaa5", "3510": "d607f94ff380497dbbf2e3811cd77d3c6113ba80", "3513": "819339e8da8325448547792a59b9656e93d17977", "3517": "5bae9ab9726635369a70a9eea95ded65215706f5", "3523": "d4e6854093138cb612a6c62b19c19a0b17d9b5b1", "3527": "70400ec7517aaf3e3b9ed598661c73c2f1a50cb1", "3534": "aa1eba344cd78605c537b37c3579664da4bc6258", "3548": "63ef514ff04a515731a0ae4b9c2eb92c1c125a8d", "3555": "9fe62421b97457f994f8d0188eebf0002038295b", "3562": "97493e6cf3b46f52204ce5ef436f1e828f6b0bb3", "3570": "8669ceee98779f78c76601272abd5663f9406f2c", "3575": "2a023239d721e69a1d645c9cd0e51232cc2815ba", "3595": "25ad2a3fded2c79b035e6bc346745863f81d3d57", "3604": "db3318090cec67f6da0812935dda29fff2605556", "3616": "07ead43a14552e2a847c253c8593a27ee508c46f", "3620": "1596c8012938bc59e0433cb56797be406cf8e445", "3638": "ef2687bf5f38e2a115f2224ab0b01a744add57e4", "3655": "3419d8f2ade78432ea180d68008e2c1a375ad08a", "3658": "2746fa0db1e8ee536bf325b328f43a8f20f94c66", "3659": "b20a6776b02e9c8120d748e85d648c26b59f0a33", "3661": "2da41171c82a027a2eadd4a0a057231d8c0eb755", "3663": "54f1979c522a7d5512926d5f8b224f5b44378317", "3667": "55e25f9205044c631c7e907e85b3d7a4047c9832", "3669": "a3f5330e47fff803fe830b416d166685ee349998", "3678": "8e4b241088f4ffd1778b0a60b935d0defe695277", "3687": "af83cfaf839507db02312e0f7402e77b0e2c87ed", "3694": "dba420f587bb8d760a4d7c89fdc60ad07a971f7f", "3696": "4719f9e87fef59b177068e6316bbc0a3eff98872", "3704": "717449fef3c05c55a02f486ee56c25f09f80e43a", "3706": "20ab093ea46c9b6e98b4a4ab9499db08f4f077a8", "3738": "b0e80e3c39ac95c3fc57ab3f040a73257cbf65fc", "3740": "db4718cc373975df3be38aacc1b967a7b29fe6fd", "3741": "c978a476b7c714bbdbb1fe2e83253002dd220772", "3744": "5d28b10ffed1a379a242242ab6a42b3422dab5ee", "3745": "1a56956ff0be53b4202dd7fbbfa4a86afd172d8e", "3747": "2032e351d53b3008165c1d6bca1157bcf339ddc8", "3749": "838804c66f6296fcb64efd068a79cc23641bed64", "3750": "56d8611b23361bebbdda20973fddf034573b6bdf", "3753": "9d073c626bd55011e1af924bd4aa5e609e277795", "3755": "85f7bf5d0ca6356de98991d9865be25f1c6b09d6", "3788": "d2ea6b112ca5c80a2ae89161c9a0067e61821169", "3792": "f4f2ddaa15a7e60c6895c2c3cb84f133f4a3adb3", "3794": "ec834d63a7067c933ce2172060238023250804b6", "3795": "f7955b31acab39f5307d9f1a7aa8261362516959", "3798": "50f1d04a514aeaaa85b0e4f5d6c35a121f6b927a", "3807": "a950d1333e34166914ba62e1a4214dbb59223675", "3809": "84678ecb47304bac0307916f75b36a5d2cc8fea3", "3813": "940f2e70ce361397a9d4abd4acc506aec2c9046a", "3821": "f24449b1aeeeb47e97f217afda8d517bac480e51", "3825": "63b8b0c553633a928e6b5cf2674913f5753a9c64", "3827": "c20c189fa835945b1cd61f48952f1dc9709638bc", "3829": "fc1d8f24c5f38bc26c01f94578ed302ed33a1164", "3831": "c9e5b04672c208f238b97825db3f6a22cff6d938", "3835": "97644bf95ff7303d0c1c10511384c303e954bf4c", "3841": "b0dad960f521565fe765eb3216c84d51010efd38", "3845": "89705d9401346164bae6ba7051d4e0526bcd6f09", "3846": "3cdb63839988378b098d0550e9edd0e53d29d111", "3848": "5b1116cdc2a0262db512091560d2af7603230fe3", "3854": "4a691baddf995159c4c8d85368bddd5317351578", "3856": "36d8b9c08ff52a4a70aa18e24aa146ba8a6c5805", "3858": "2e60517cb699d183f2c1338b1b10568abc89f808", "3859": "df863b0e93095648cad61073b07a12625baf8415", "3860": "7774c90642fceb5575e54692ae4398cd64bf4959", "3861": "b82bec2fa50c9966ceefee43c7ec4b3d3f48bc74", "3864": "fd0ec5d0800d9e87de32f9d07a1dc1dcbc137ed0", "3866": "501c0c17091d7efe8db9a3e68989c365a4e2e109", "3871": "9142dd4c72cc0cbec18e8c05c9b5d7fd4197b117", "3873": "f9ecab1a0ab37ca08ec2b8acea90a26dd5345b63", "3874": "8ee376319e69e82ad7e2ce1ce02758b71905c668", "3875": "600bd5a888aaec0bfacc3750553f7b03d1b22816", "3876": "0b9618581fafb726e65e56778e5011ec36380473", "3878": "c3156f020d9f7b511f46c528c65c2c6b1b28b5fc", "3882": "14ca39fb569343bd71f0647561d84173eefe1b50", "3892": "1132b9674d00defae4de7964c419cbdd6b8ed01f", "3898": "e96437d12ae18590ded9e806f5f0c8bf03b98ef8", "3902": "f1948093dd91eb1a20a01837d491947f5a689f0f", "3903": "b1dd059d5e313a7c287d445ae47ee31babae4a69", "3905": "9f2ddbaadc7337051de966ad441c65f4fc89ef77", "3906": "d8b855dffe518d22b524835312a255cdd5084b7d", "3907": "bc71467f61ee3017325ad4ee037faff0a845e7aa", "3909": "26c056f9a38ef18d9db2fca4471823b36f13fc36", "3911": "e140bca76d0a65a2b7879e4a6862cee350b17114"}, "revision_to_date": {"42": 1678962043000, "318": 1683553743000, "457": 1684752617000, "480": 1685003160000, "637": 1686329453000, "727": 1687514692000, "735": 1687872752000, "737": 1687875060000, "739": 1687875381000, "811": 1689023430000, "884": 1690274961000, "912": 1690383063000, "927": 1690877437000, "942": 1691153153000, "970": 1691507415000, "1028": 1692029284000, "1103": 1693225318000, "1194": 1693832948000, "1206": 1693959700000, "1224": 1694183836000, "1225": 1694192794000, "1242": 1694691587000, "1278": 1695302465000, "1300": 1695645568000, "1312": 1695748368000, "1336": 1695993535000, "1354": 1696342948000, "1369": 1696861881000, "1435": 1697796332000, "1509": 1698689107000, "1518": 1698773717000, "1522": 1698788569000, "1537": 1698844752000, "1538": 1698846082000, "1661": 1700501108000, "1704": 1700745145000, "1713": 1700831897000, "1890": 1702383559000, "1914": 1702566689000, "2108": 1705055585000, "2130": 1705330362000, "2255": 1706109394000, "2304": 1706519986000, "2310": 1706624458000, "2355": 1707226614000, "2356": 1707227524000, "2404": 1707409032000, "2431": 1707819609000, "2507": 1708946786000, "2509": 1708947852000, "2537": 1709033907000, "2641": 1710379468000, "2729": 1711382694000, "2738": 1711611193000, "2759": 1712137847000, "2763": 1712215927000, "2791": 1713175869000, "2883": 1714565416000, "2888": 1714829539000, "2889": 1714495986000, "2892": 1715088318000, "2898": 1715251867000, "2900": 1715273959000, "2907": 1715370783000, "2912": 1715672695000, "2918": 1715777261000, "2925": 1715784844000, "2926": 1712846843000, "2935": 1715950456000, "2944": 1716292993000, "2947": 1716453922000, "2956": 1716997758000, "2958": 1717087554000, "2961": 1717170495000, "2974": 1717414433000, "2976": 1717414901000, "2978": 1717505162000, "2981": 1717513301000, "3009": 1717592919000, "3016": 1718036933000, "3027": 1718120600000, "3033": 1718196636000, "3039": 1718291838000, "3050": 1718729150000, "3057": 1718802776000, "3058": 1718792786000, "3069": 1718980969000, "3071": 1719219102000, "3085": 1719324822000, "3088": 1719326066000, "3092": 1719403203000, "3104": 1719565491000, "3112": 1719590331000, "3115": 1719779356000, "3118": 1719857742000, "3124": 1719997980000, "3131": 1720163872000, "3135": 1720442209000, "3140": 1720508932000, "3148": 1720715859000, "3150": 1720770981000, "3171": 1721060703000, "3182": 1721117007000, "3193": 1721321907000, "3197": 1721729324000, "3203": 1721744859000, "3228": 1721812779000, "3230": 1721819059000, "3231": 1721825228000, "3236": 1722001245000, "3239": 1722271303000, "3250": 1722346568000, "3262": 1722432803000, "3274": 1722514049000, "3276": 1722514119000, "3288": 1722592500000, "3291": 1722602989000, "3312": 1722855272000, "3318": 1722935151000, "3319": 1722939960000, "3327": 1722440540000, "3331": 1723236605000, "3341": 1723308188000, "3345": 1723545798000, "3366": 1723834000000, "3370": 1724088393000, "3376": 1724164405000, "3387": 1724252182000, "3399": 1719933161000, "3405": 1724417246000, "3417": 1724832448000, "3427": 1724840138000, "3433": 1725286457000, "3443": 1725284399000, "3456": 1725474667000, "3460": 1724658986000, "3462": 1725548928000, "3463": 1725554020000, "3473": 1725887807000, "3481": 1725973036000, "3487": 1725979620000, "3489": 1726036670000, "3491": 1726248025000, "3495": 1726508701000, "3503": 1726755925000, "3507": 1727080005000, "3510": 1727161687000, "3513": 1727187284000, "3517": 1727339498000, "3523": 1727437221000, "3527": 1727688292000, "3534": 1727781021000, "3548": 1727792229000, "3555": 1728043587000, "3562": 1728297449000, "3570": 1728563742000, "3575": 1728625775000, "3595": 1728660284000, "3604": 1728639898000, "3616": 1728983385000, "3620": 1726151644000, "3638": 1729091189000, "3655": 1729100594000, "3658": 1729157707000, "3659": 1729173033000, "3661": 1729188377000, "3663": 1729245260000, "3667": 1729497839000, "3669": 1726151644000, "3678": 1729669684000, "3687": 1729759206000, "3694": 1729854933000, "3696": 1729878319000, "3704": 1730119639000, "3706": 1730119739000, "3738": 1730122298000, "3740": 1726125290000, "3741": 1727439740000, "3744": 1728569151000, "3745": 1728899271000, "3747": 1730221470000, "3749": 1730272822000, "3750": 1730281261000, "3753": 1730296936000, "3755": 1730301130000, "3788": 1730377229000, "3792": 1730382436000, "3794": 1728479591000, "3795": 1730389272000, "3798": 1730799065000, "3807": 1730812670000, "3809": 1730477115000, "3813": 1730822184000, "3821": 1730819209000, "3825": 1730889307000, "3827": 1730972004000, "3829": 1730989718000, "3831": 1731075419000, "3835": 1731147566000, "3841": 1731407686000, "3845": 1731499132000, "3846": 1731502029000, "3848": 1731509471000, "3854": 1731517977000, "3856": 1731577408000, "3858": 1731595221000, "3859": 1731595675000, "3860": 1731661004000, "3861": 1731665419000, "3864": 1731675638000, "3866": 1731683684000, "3871": 1731653776000, "3873": 1731932289000, "3874": 1731941827000, "3875": 1731997480000, "3876": 1732023861000, "3878": 1732087066000, "3882": 1732184398000, "3892": 1732296908000, "3898": 1732534292000, "3902": 1729765965000, "3903": 1732542060000, "3905": 1732550672000, "3906": 1732550356000, "3907": 1732554558000, "3909": 1732558866000, "3911": 1732289228000}, "params": {"machine": ["ArcticDB-Medium-Runner"], "python": ["3.6"], "version": [1, null], "branch": ["master"]}, "graph_param_list": [{"machine": "ArcticDB-Medium-Runner", "python": "3.6", "branch": "master", "version": null}], "benchmarks": {"basic_functions.BasicFunctions.peakmem_read": {"code": "class BasicFunctions:\n    def peakmem_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "2aac10521743bd6cc38b0651c5889d474b5468c1166994261a08aa12c090e354"}, "basic_functions.BasicFunctions.peakmem_read_short_wide": {"code": "class BasicFunctions:\n    def peakmem_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "8dc0558241a4d313bcc9c607e79ceae1848864130859c3d5512889303c5b1ab2"}, "basic_functions.BasicFunctions.peakmem_read_with_columns": {"code": "class BasicFunctions:\n    def peakmem_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_with_columns", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "514794820c0bbf4a1c82aaaf47090ca4ed7a6834bc59a8d30757623798f409f7"}, "basic_functions.BasicFunctions.peakmem_read_with_date_ranges": {"code": "class BasicFunctions:\n    def peakmem_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_read_with_date_ranges", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "a88b0d9a342fd1be6fd7564bfcac0597bd9046ebe0d4ddd01ef4c10ac95326f6"}, "basic_functions.BasicFunctions.peakmem_write": {"code": "class BasicFunctions:\n    def peakmem_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "8b2b9b8fa4fff3c9b39e8282371bbb03daf0cfda234931598dc9103ef557e87c"}, "basic_functions.BasicFunctions.peakmem_write_short_wide": {"code": "class BasicFunctions:\n    def peakmem_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write_short_wide", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "22e1d7860c2f63edbe004b56bb87f88495e8b012c7b802a4ff08a12b141c434a"}, "basic_functions.BasicFunctions.peakmem_write_staged": {"code": "class BasicFunctions:\n    def peakmem_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "name": "basic_functions.BasicFunctions.peakmem_write_staged", "param_names": ["rows"], "params": [["1000000", "1500000"]], "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "7292f3cb4cf929adc3a00d2fa29bc2c8e1472bd52f1001564cc652d0a9433b04"}, "basic_functions.BasicFunctions.time_read": {"code": "class BasicFunctions:\n    def time_read(self, rows):\n        self.lib.read(f\"sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c9e34f0d193d0b815bb954c03915b389b6e05cdfc18545517b411e335e14e3f8", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_short_wide": {"code": "class BasicFunctions:\n    def time_read_short_wide(self, rows):\n        lib = self.ac[get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)]\n        lib.read(\"short_wide_sym\").data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8845b4655ab7a2fc0b836da115b599fe8bb7ba8001660b6af543acb5fe765170", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_with_columns": {"code": "class BasicFunctions:\n    def time_read_with_columns(self, rows):\n        COLS = [\"value\"]\n        self.lib.read(f\"sym\", columns=COLS).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_columns", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "edfa58702d2a4ee0ee84197a7ba81194e5e3aecc0f8520f2a8d4b06d45896021", "warmup_time": -1}, "basic_functions.BasicFunctions.time_read_with_date_ranges": {"code": "class BasicFunctions:\n    def time_read_with_date_ranges(self, rows):\n        self.lib.read(f\"sym\", date_range=BasicFunctions.DATE_RANGE).data\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_read_with_date_ranges", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "111bbd81442ec1a42e9c28ce665a629c3875d7c4dc1e6771f4cdce9fed080e2b", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write": {"code": "class BasicFunctions:\n    def time_write(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5be801860fd4852357fa9010f680596a23a606e1f85e672341de2a1b472ce1e1", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write_short_wide": {"code": "class BasicFunctions:\n    def time_write_short_wide(self, rows):\n        self.fresh_lib.write(\"short_wide_sym\", self.df_short_wide)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_short_wide", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "25b8f595dd9b3b81d5544e539fb0f4c24455015c2ccb2bceb82c28fbf0698680", "warmup_time": -1}, "basic_functions.BasicFunctions.time_write_staged": {"code": "class BasicFunctions:\n    def time_write_staged(self, rows):\n        self.fresh_lib.write(f\"sym\", self.df, staged=True)\n        self.fresh_lib._nvs.compact_incomplete(f\"sym\", False, False)\n\n    def setup(self, rows):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.df_short_wide = generate_random_floats_dataframe(\n            BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n        )\n    \n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BasicFunctions.CONNECTION_STRING)\n        rows_values = BasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            lib.write(f\"sym\", self.dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(BasicFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe(\n                BasicFunctions.WIDE_DF_ROWS, BasicFunctions.WIDE_DF_COLS\n            ),\n        )", "min_run_count": 2, "name": "basic_functions.BasicFunctions.time_write_staged", "number": 5, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "4e330332407ef3b614f0cafbed8ff8fd944b2ecf9854ab225cef0cda6bb8ce8c", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.peakmem_read_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "8fe50f627f8d9efb117cadb0f26ee0cbe39662ba1b2905cf3984f33fb03b6f6e"}, "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_columns", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "b260f7db89580432675d89ba8d08e74ff830f74b81cf7803d58ba44068c9a49c"}, "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges": {"code": "class BatchBasicFunctions:\n    def peakmem_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE)\n            for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_read_batch_with_date_ranges", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "f440048c530dbb90f782985f7c45e774e3cda60f0d134665e4ef6adf02bb18b7"}, "basic_functions.BatchBasicFunctions.peakmem_write_batch": {"code": "class BatchBasicFunctions:\n    def peakmem_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "name": "basic_functions.BatchBasicFunctions.peakmem_write_batch", "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "add75de5523f5002ae08e9f3c01ab33698a49635fe16d01031164b203e4fdfe1"}, "basic_functions.BatchBasicFunctions.time_read_batch": {"code": "class BatchBasicFunctions:\n    def time_read_batch(self, rows, num_symbols):\n        read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "32b2eed380d4e7863cb87b2011f6add3881e424eeae2cd9b11dc75507fbf8640", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.time_read_batch_pure": {"code": "class BatchBasicFunctions:\n    def time_read_batch_pure(self, rows, num_symbols):\n        self.lib.read_batch(self.read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_pure", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "2d1bcf173d39e007c6e55c195e669dc59c57c6852e3ec435d30e421f6baa606e", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.time_read_batch_with_columns": {"code": "class BatchBasicFunctions:\n    def time_read_batch_with_columns(self, rows, num_symbols):\n        COLS = [\"value\"]\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", columns=COLS) for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_columns", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ee90e0674ceeebbac5b24babbd514a31eecf352b2fd8d232fce2fe886c559da3", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges": {"code": "class BatchBasicFunctions:\n    def time_read_batch_with_date_ranges(self, rows, num_symbols):\n        read_reqs = [\n            ReadRequest(f\"{sym}_sym\", date_range=BatchBasicFunctions.DATE_RANGE)\n            for sym in range(num_symbols)\n        ]\n        self.lib.read_batch(read_reqs)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_read_batch_with_date_ranges", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9ce9e0532a98436b27f547980b8b4ec1914e34acab556ec30e8f3727daf7290d", "warmup_time": -1}, "basic_functions.BatchBasicFunctions.time_write_batch": {"code": "class BatchBasicFunctions:\n    def time_write_batch(self, rows, num_symbols):\n        payloads = [WritePayload(f\"{sym}_sym\", self.df) for sym in range(num_symbols)]\n        self.fresh_lib.write_batch(payloads)\n\n    def setup(self, rows, num_symbols):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        self.read_reqs = [ReadRequest(f\"{sym}_sym\") for sym in range(num_symbols)]\n    \n        self.df = generate_pseudo_random_dataframe(rows)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.fresh_lib = self.get_fresh_lib()\n\n    def setup_cache(self):\n        self.ac = Arctic(BatchBasicFunctions.CONNECTION_STRING)\n        rows_values, num_symbols_values = BatchBasicFunctions.params\n    \n        self.dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib)\n            self.ac.create_library(lib)\n            lib = self.ac[lib]\n            for sym in range(num_symbols_values[-1]):\n                lib.write(f\"{sym}_sym\", self.dfs[rows])", "min_run_count": 2, "name": "basic_functions.BatchBasicFunctions.time_write_batch", "number": 5, "param_names": ["rows", "num_symbols"], "params": [["25000", "50000"], ["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:137", "timeout": 6000, "type": "time", "unit": "seconds", "version": "6989b158867091ab3cb5c68d4d6418425164561776e8b86e37c24e635dd3b722", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_append_large": {"code": "class ModificationFunctions:\n    def time_append_large(self, rows):\n        self.lib.append(f\"sym\", self.df_append_large)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_large", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "be3be12028b2f1a949589e618252e94a88e5f35b5aa90f5815fd8aaa324c8550", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_append_short_wide": {"code": "class ModificationFunctions:\n    def time_append_short_wide(self, rows):\n        self.lib_short_wide.append(\"short_wide_sym\", self.df_append_short_wide)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "3a2e1e7a4dc518468ba388f560231ac1a1366b212dbd3309e3e877606c5630e8", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_append_single": {"code": "class ModificationFunctions:\n    def time_append_single(self, rows):\n        self.lib.append(f\"sym\", self.df_append_single)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_append_single", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c7f13a15b9074ab9bdb6f3e47ab97d75708938f005021b7a8fde82fe6902041d", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_delete": {"code": "class ModificationFunctions:\n    def time_delete(self, rows):\n        self.lib.delete(f\"sym\")\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "da4c95139bc0ae404ed6585b9e3398af8ed7e421cefcbeb9ff9ea6a77b85915a", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_delete_short_wide": {"code": "class ModificationFunctions:\n    def time_delete_short_wide(self, rows):\n        self.lib_short_wide.delete(\"short_wide_sym\")\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_delete_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "12254786f4a42e8bd488f48075cb70eddf4d87c8581271e2e2b526b7940123b9", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_update_half": {"code": "class ModificationFunctions:\n    def time_update_half(self, rows):\n        self.lib.update(f\"sym\", self.df_update_half)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_half", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "f56b8677f5b90b49568e6865c0656b734b9b2a8054baa71b78eaed8f53cb3176", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_update_short_wide": {"code": "class ModificationFunctions:\n    def time_update_short_wide(self, rows):\n        self.lib_short_wide.update(\"short_wide_sym\", self.df_update_short_wide)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_short_wide", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5db16777228d8de1ab4af9943d1ed0541c0b02c4dbcd888cfa3e26f37eb0215b", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_update_single": {"code": "class ModificationFunctions:\n    def time_update_single(self, rows):\n        self.lib.update(f\"sym\", self.df_update_single)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_single", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "cf62fa8a658e2f2ab16d286992423dd8d69334415ab61600906c6e9dc0185597", "warmup_time": -1}, "basic_functions.ModificationFunctions.time_update_upsert": {"code": "class ModificationFunctions:\n    def time_update_upsert(self, rows):\n        self.lib.update(f\"sym\", self.df_update_upsert, upsert=True)\n\n    def setup(self, rows):\n        def get_time_at_fraction_of_df(fraction, rows=rows):\n            end_time = pd.Timestamp(\"1/1/2023\")\n            time_delta = pd.tseries.offsets.DateOffset(seconds=round(rows * (fraction-1)))\n            return end_time + time_delta\n    \n        self.df_update_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(0.5))\n        self.df_update_half = generate_pseudo_random_dataframe(rows//2, \"s\", get_time_at_fraction_of_df(0.75))\n        self.df_update_upsert = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(1.5))\n        self.df_append_single = generate_pseudo_random_dataframe(1, \"s\", get_time_at_fraction_of_df(1.1))\n        self.df_append_large = generate_pseudo_random_dataframe(rows, \"s\", get_time_at_fraction_of_df(2))\n    \n        self.df_update_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n        )\n        self.df_append_short_wide = generate_random_floats_dataframe_with_index(\n            ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS, \"s\", get_time_at_fraction_of_df(2, rows=ModificationFunctions.WIDE_DF_ROWS)\n        )\n    \n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        self.lib = self.ac[get_prewritten_lib_name(rows)]\n        self.lib_short_wide = self.ac[get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)]\n\n    def setup_cache(self):\n        self.ac = Arctic(ModificationFunctions.CONNECTION_STRING)\n        rows_values = ModificationFunctions.params\n    \n        self.init_dfs = {rows: generate_pseudo_random_dataframe(rows) for rows in rows_values}\n        for rows in rows_values:\n            lib_name = get_prewritten_lib_name(rows)\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            lib.write(\"sym\", self.init_dfs[rows])\n    \n        lib_name = get_prewritten_lib_name(ModificationFunctions.WIDE_DF_ROWS)\n        self.ac.delete_library(lib_name)\n        lib = self.ac.create_library(lib_name)\n        lib.write(\n            \"short_wide_sym\",\n            generate_random_floats_dataframe_with_index(\n                ModificationFunctions.WIDE_DF_ROWS, ModificationFunctions.WIDE_DF_COLS\n            ),\n        )\n    \n        # We use the fact that we're running on LMDB to store a copy of the initial arctic directory.\n        # Then on each teardown we restore the initial state by overwriting the modified with the original.\n        copytree(ModificationFunctions.ARCTIC_DIR, ModificationFunctions.ARCTIC_DIR_ORIGINAL)", "min_run_count": 2, "name": "basic_functions.ModificationFunctions.time_update_upsert", "number": 1, "param_names": ["rows"], "params": [["1000000", "1500000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "basic_functions:235", "timeout": 6000, "type": "time", "unit": "seconds", "version": "80de9b1982a498c300177d02874a8626152eccb57cd0ba4228a5bb168e7608c8", "warmup_time": -1}, "list_functions.ListFunctions.peakmem_list_symbols": {"code": "class ListFunctions:\n    def peakmem_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "name": "list_functions.ListFunctions.peakmem_list_symbols", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "efa8557e59868203fde3f8d2921698b505ae7a1ce7ff442b3e4c9bebc9ce2771"}, "list_functions.ListFunctions.peakmem_list_versions": {"code": "class ListFunctions:\n    def peakmem_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "name": "list_functions.ListFunctions.peakmem_list_versions", "param_names": ["num_symbols"], "params": [["500", "1000"]], "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "30457537b6ea77365ec0021b6f78a618dd0e990631d64cf0ae6b85baddca7081"}, "list_functions.ListFunctions.time_has_symbol": {"code": "class ListFunctions:\n    def time_has_symbol(self, num_symbols):\n        self.lib.has_symbol(\"250_sym\")\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_has_symbol", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "00a6aba7cd18f9fbbfa18c85961d58a03a291bfe32bf033e8d7b88c7b960da90", "warmup_time": -1}, "list_functions.ListFunctions.time_list_symbols": {"code": "class ListFunctions:\n    def time_list_symbols(self, num_symbols):\n        self.lib.list_symbols()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_symbols", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "7457ceb57b7adfda687387a4599ff60b20ecb6ef556b80329ad2e8ec433fbb17", "warmup_time": -1}, "list_functions.ListFunctions.time_list_versions": {"code": "class ListFunctions:\n    def time_list_versions(self, num_symbols):\n        self.lib.list_versions()\n\n    def setup(self, num_symbols):\n        self.ac = Arctic(\"lmdb://list_functions\")\n        self.lib = self.ac[f\"{num_symbols}_num_symbols\"]\n\n    def setup_cache(self):\n        self.ac = Arctic(\"lmdb://list_functions\")\n    \n        num_symbols = ListFunctions.params\n        for syms in num_symbols:\n            lib_name = f\"{syms}_num_symbols\"\n            self.ac.delete_library(lib_name)\n            lib = self.ac.create_library(lib_name)\n            for sym in range(syms):\n                lib.write(f\"{sym}_sym\", generate_benchmark_df(ListFunctions.rows))", "min_run_count": 2, "name": "list_functions.ListFunctions.time_list_versions", "number": 5, "param_names": ["num_symbols"], "params": [["500", "1000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "list_functions:22", "timeout": 6000, "type": "time", "unit": "seconds", "version": "cc2c68ce66d0087882fffcb8be554f525c3f314c8693a37897d37cc18373f1ff", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 10.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_numeric", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "1f8093c32e1c5195eb0efb1004c228524cb54aa35d8c79359b17fc91597391a6"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_filtering_string_isin", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "fa0a87f2f2956453b825adcdb9e95be6a7e8887b2a66839923aa8a433e296e4e"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_projection", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "ce91e45ba6ec5f5dcd9499b423014b431774a7d81f07daa90d6c29cb8bc84d02"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_1", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "4cad6e9389f20fc4a168893003dff16e0577770525b847e71e3b97f0f9f5ecdd"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_3", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "91dfe91e2fd6e9d562d89d8aee902dbb5c2380f3cd0a11eb85229cb375a7ea0b"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_4", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "94edfd985cb9746d21b85be1c91e97423797af2faa7a3343ad1c3aa7f9fa4536"}, "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def peakmem_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "name": "local_query_builder.LocalQueryBuilderFunctions.peakmem_query_adv_query_2", "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "peakmemory", "unit": "bytes", "version": "e047abda334f31dda20959739f2a3816f4dc96c130db00ebb75f5adcb9c14999"}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_numeric(self, num_rows):\n        q = QueryBuilder()\n        # v3 is random floats between 0 and 100\n        q = q[q[\"v3\"] < 1.0]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_numeric", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "1fd26d5df8e3bd47278b0f1acca9528cc0dadba82788af6e3cfd1812058abef9", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin": {"code": "class LocalQueryBuilderFunctions:\n    def time_filtering_string_isin(self, num_rows):\n        # Selects about 1% of the rows\n        k = num_rows // 1000\n        string_set = [f\"id{str(i).zfill(3)}\" for i in range(1, k + 1)]\n        q = QueryBuilder()\n        q = q[q[\"id1\"].isin(string_set)]\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"v3\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_filtering_string_isin", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "a0f79b58b7744e63b2b7df3562f57094fa4ff3a111c172fbe0b03aec197afec8", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_projection": {"code": "class LocalQueryBuilderFunctions:\n    def time_projection(self, num_rows):\n        q = QueryBuilder()\n        q = q.apply(\"new_col\", q[\"v2\"] * q[\"v3\"])\n        self.lib.read(f\"{num_rows}_rows\", columns=[\"new_col\"], query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_projection", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c7f842a915ebd3e278a9a5cea838835a804b463451ebec69829afe871adccfcc", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_1": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_1", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8972136efca70caee7530d031766c4653737a79d09b7c7badaaee274c1caa7da", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_3": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_3", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "17ef74af58c623de0ce47d10ad9d52ffc8a1b3c3bb2f57d1391dde34f4af4f29", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_4": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_4", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "509ffd471564124f5ea73eab19903e52e70eba728ea59b97ad6bd5b8544c2e60", "warmup_time": -1}, "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2": {"code": "class LocalQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n        self.lib = self.ac[LocalQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(LocalQueryBuilderFunctions.CONNECTION_STRING)\n    \n        num_rows = LocalQueryBuilderFunctions.params\n        self.lib_name = LocalQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "local_query_builder.LocalQueryBuilderFunctions.time_query_adv_query_2", "number": 5, "param_names": ["num_rows"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "local_query_builder:23", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9a923014466d420b857d297f2a8a41983d03d0c3242559a8488a2a9a642440e1", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_1(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id1\").agg({\"v1\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_1", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:62", "timeout": 6000, "type": "time", "unit": "seconds", "version": "9d97dcd98574b9edb2038a9d43166c03fb90874813e5fac9c3a44b51194f3dd9", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_3(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"sum\", \"v3\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_3", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:62", "timeout": 6000, "type": "time", "unit": "seconds", "version": "b1364bf72e616201e384c0b7a9f18b03b078e22452929466a06b35fc64a91bd6", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_4(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id6\").agg({\"v1\": \"sum\", \"v2\": \"sum\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_4", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:62", "timeout": 6000, "type": "time", "unit": "seconds", "version": "8f27fb785c7b8b40220191dae6dbb120a49f55e011ae0f7cea6516a47e38c18a", "warmup_time": -1}, "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2": {"code": "class PersistentQueryBuilderFunctions:\n    def time_query_adv_query_2(self, num_rows):\n        q = QueryBuilder()\n        q = q.groupby(\"id3\").agg({\"v1\": \"max\", \"v2\": \"min\"})\n        self.lib.read(f\"{num_rows}_rows\", query_builder=q)\n\n    def setup(self, num_rows):\n        self.lib = self.ac[PersistentQueryBuilderFunctions.LIB_NAME]\n\n    def setup_cache(self):\n        self.ac = Arctic(get_real_s3_uri())\n    \n        num_rows = PersistentQueryBuilderFunctions.params\n        self.lib_name = PersistentQueryBuilderFunctions.LIB_NAME\n        self.ac.delete_library(self.lib_name)\n        lib = self.ac.create_library(self.lib_name)\n        for rows in num_rows:\n            lib.write(f\"{rows}_rows\", generate_benchmark_df(rows))", "min_run_count": 2, "name": "persistent_query_builder.PersistentQueryBuilderFunctions.time_query_adv_query_2", "number": 2, "param_names": ["param1"], "params": [["1000000", "10000000"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "persistent_query_builder:62", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ed1d1ccb6458095a627788bfa2b53afa310ca8c8118a6405c91204724c865d6c", "warmup_time": -1}, "resample.Resample.peakmem_resample": {"code": "class Resample:\n    def peakmem_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)", "name": "resample.Resample.peakmem_resample", "param_names": ["num_rows", "downsampling_factor", "col_type", "aggregation"], "params": [["1000000", "10000000"], ["10", "100", "100000"], ["'bool'", "'int'", "'float'", "'datetime'", "'str'"], ["'sum'", "'mean'", "'min'", "'max'", "'first'", "'last'", "'count'"]], "setup_cache_key": "resample:38", "type": "peakmemory", "unit": "bytes", "version": "e64300ebb5bd625e1a0f3774aadd035e5738b41295ec2a8ce082d2e9add9b580"}, "resample.Resample.time_resample": {"code": "class Resample:\n    def time_resample(self, num_rows, downsampling_factor, col_type, aggregation):\n        if col_type == \"datetime\" and aggregation == \"sum\" or col_type == \"str\" and aggregation in [\"sum\", \"mean\", \"min\", \"max\"]:\n            raise SkipNotImplemented(f\"{aggregation} not supported on columns of type {col_type}\")\n        else:\n            self.lib.read(col_type, date_range=self.date_range, query_builder=self.query_builder)\n\n    def setup(self, num_rows, downsampling_factor, col_type, aggregation):\n        self.ac = Arctic(self.CONNECTION_STRING)\n        self.lib = self.ac[self.LIB_NAME]\n        self.date_range = (pd.Timestamp(0), pd.Timestamp(num_rows, unit=\"us\"))\n        self.query_builder = QueryBuilder().resample(f\"{downsampling_factor}us\").agg({\"col\": aggregation})\n\n    def setup_cache(self):\n        ac = Arctic(self.CONNECTION_STRING)\n        ac.delete_library(self.LIB_NAME)\n        lib = ac.create_library(self.LIB_NAME)\n        rng = np.random.default_rng()\n        col_types = self.params[2]\n        rows = max(self.params[0])\n        for col_type in col_types:\n            if col_type == \"str\":\n                num_unique_strings = 100\n                unique_strings = random_strings_of_length(num_unique_strings, 10, True)\n            sym = col_type\n            num_segments = rows // self.ROWS_PER_SEGMENT\n            for idx in range(num_segments):\n                index = pd.date_range(pd.Timestamp(idx * self.ROWS_PER_SEGMENT, unit=\"us\"), freq=\"us\", periods=self.ROWS_PER_SEGMENT)\n                if col_type == \"int\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                elif col_type == \"bool\":\n                    col_data = rng.integers(0, 2, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(bool)\n                elif col_type == \"float\":\n                    col_data = 100_000 * rng.random(self.ROWS_PER_SEGMENT)\n                elif col_type == \"datetime\":\n                    col_data = rng.integers(0, 100_000, self.ROWS_PER_SEGMENT)\n                    col_data = col_data.astype(\"datetime64[s]\")\n                elif col_type == \"str\":\n                    col_data = np.random.choice(unique_strings, self.ROWS_PER_SEGMENT)\n                df = pd.DataFrame({\"col\": col_data}, index=index)\n                lib.append(sym, df)", "min_run_count": 2, "name": "resample.Resample.time_resample", "number": 5, "param_names": ["num_rows", "downsampling_factor", "col_type", "aggregation"], "params": [["1000000", "10000000"], ["10", "100", "100000"], ["'bool'", "'int'", "'float'", "'datetime'", "'str'"], ["'sum'", "'mean'", "'min'", "'max'", "'first'", "'last'", "'count'"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "resample:38", "type": "time", "unit": "seconds", "version": "2d10a27f3668632f382e90783829b4bb08cabb656c02754c00d5953ee42f3794", "warmup_time": -1}, "version_chain.IterateVersionChain.time_list_undeleted_versions": {"code": "class IterateVersionChain:\n    def time_list_undeleted_versions(self, num_versions, caching, deleted):\n        self.lib.list_versions(symbol=self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_list_undeleted_versions", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "6bdd43d7f191d2bbbd30ef740909969e25cbe1cec77f1755c5c3ba58a77f2b88", "warmup_time": -1}, "version_chain.IterateVersionChain.time_load_all_versions": {"code": "class IterateVersionChain:\n    def time_load_all_versions(self, num_versions, caching, deleted):\n        self.load_all(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_load_all_versions", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "c40fe3123db9e5d6fdf5f35caecaf42d266328deb78c237e293096ae3a4bcf98", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_alternating": {"code": "class IterateVersionChain:\n    def time_read_alternating(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_alternating", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "ec1a61c37c4cc7317cfafe554f3eeb7fe2a426068ec412c1d7c6b78f510f6c45", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_from_epoch": {"code": "class IterateVersionChain:\n    def time_read_from_epoch(self, num_versions, caching, deleted):\n        self.read_from_epoch(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_from_epoch", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "5c6aace0b39c7a75f064a61c182cbbb42a35f0e0ee46546579bc641e68dc954a", "warmup_time": -1}, "version_chain.IterateVersionChain.time_read_v0": {"code": "class IterateVersionChain:\n    def time_read_v0(self, num_versions, caching, deleted):\n        self.read_v0(self.symbol(num_versions, deleted))\n\n    def setup(self, num_versions, caching, deleted):\n        # Disable warnings for version not found\n        set_log_level(\"ERROR\")\n    \n        if caching==\"never\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", 0)\n        if caching==\"forever\":\n            adb._ext.set_config_int(\"VersionMap.ReloadInterval\", sys.maxsize)\n        if caching==\"default\":\n            # Leave the default reload interval\n            pass\n    \n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        self.lib = self.ac[IterateVersionChain.LIB_NAME]\n    \n        if caching != \"never\":\n            # Pre-load the cache\n            self.load_all(self.symbol(num_versions, deleted))\n\n    def setup_cache(self):\n        self.ac = Arctic(IterateVersionChain.CONNECTION_STRING)\n        num_versions_list, caching_list, deleted_list = IterateVersionChain.params\n    \n        self.ac.delete_library(IterateVersionChain.LIB_NAME)\n        lib = self.ac.create_library(IterateVersionChain.LIB_NAME)\n    \n        small_df = generate_random_floats_dataframe(2, 2)\n    \n        for num_versions in num_versions_list:\n            for deleted in deleted_list:\n                symbol = self.symbol(num_versions, deleted)\n                for i in range(num_versions):\n                    lib.write(symbol, small_df)\n                    if (i == math.floor(deleted * num_versions)):\n                        lib.delete(symbol)\n    \n        del self.ac", "min_run_count": 2, "name": "version_chain.IterateVersionChain.time_read_v0", "number": 10, "param_names": ["num_versions", "caching", "deleted"], "params": [["25000"], ["'forever'", "'default'", "'never'"], ["0.0", "0.99"]], "repeat": 0, "rounds": 2, "sample_time": 0.01, "setup_cache_key": "version_chain:36", "timeout": 6000, "type": "time", "unit": "seconds", "version": "4bf693e490128c1cff7500c93799432e7bf150925d3714757219604aa7fa5e9c", "warmup_time": -1}}, "machines": {"ArcticDB-Medium-Runner": {"machine": "ArcticDB-Medium-Runner", "python": "3.6", "version": 1}}, "tags": {"1.0.1": 42, "v0.0.0": 3658, "v0.0.1": 3663, "v0.0.2": 3667, "v1.0.1": 42, "v1.1.0": 318, "v1.2.0": 457, "v1.2.1": 480, "v1.3.0": 637, "v1.4.0": 727, "v1.4.1": 735, "v1.4.1-pre.seatontst": 737, "v1.4.1-pre.seatontst.2": 739, "v1.5.0": 811, "v1.6.0": 884, "v1.6.1": 970, "v1.6.1-rc0": 912, "v1.6.1-rc1": 927, "v1.6.1rc2": 942, "v1.6.2": 1354, "v1.6.2rc0": 1336, "v2.0.0": 1103, "v2.0.0rc0": 1028, "v3.0.0": 1224, "v3.0.0rc0": 1194, "v3.0.0rc1": 1206, "v3.0.0rc2": 1225, "v4.0.0": 1312, "v4.0.0rc0": 1242, "v4.0.0rc1": 1278, "v4.0.0rc2": 1300, "v4.0.1": 1435, "v4.0.2": 1537, "v4.0.2rc0": 1518, "v4.0.3": 1704, "v4.0.4": 2641, "v4.0.4-docs": 2641, "v4.0.5": 2981, "v4.1.0": 1538, "v4.1.0-docs": 1661, "v4.1.0rc0": 1369, "v4.1.0rc1": 1509, "v4.1.0rc2": 1522, "v4.2.0": 1890, "v4.2.0-docs": 1890, "v4.2.0rc0": 1713, "v4.2.1": 1914, "v4.2.1-docs": 2130, "v4.3.0": 2355, "v4.3.0-docs": 2310, "v4.3.0rc0": 2255, "v4.3.0rc1": 2304, "v4.3.0rc2": 2356, "v4.3.1": 2404, "v4.3.1-docs": 2431, "v4.3.2rc0": 2507, "v4.3.2rc1": 2509, "v4.3.2rc2": 2537, "v4.4.0": 2763, "v4.4.0-docs": 2763, "v4.4.0rc0": 2729, "v4.4.0rc1": 2738, "v4.4.0rc2": 2759, "v4.4.1": 2791, "v4.4.1-docs": 2791, "v4.4.2": 2892, "v4.4.2-docs": 2892, "v4.4.2rc0": 2888, "v4.4.3": 3057, "v4.4.3-docs": 3057, "v4.4.3rc0": 2925, "v4.4.3rc1": 2976, "v4.4.4": 3462, "v4.4.4rc0": 3085, "v4.4.4rc1": 3115, "v4.4.4rc2": 3140, "v4.4.4rc3": 3197, "v4.4.4rc4": 3230, "v4.4.4rc5": 3288, "v4.4.4rc6": 3319, "v4.4.5": 3489, "v4.4.6": 3562, "v4.5.0": 3345, "v4.5.0-docs": 3345, "v4.5.0rc0": 3071, "v4.5.0rc1": 3104, "v4.5.0rc2": 3231, "v4.5.0rc3": 3276, "v4.5.0rc4": 3318, "v4.5.1": 3661, "v4.5.1-docs": 3661, "v4.5.1rc0": 3503, "v4.5.1rc1": 3513, "v4.5.1rc2": 3575, "v4.5.1rc3": 3616, "v4.5.1rc4": 3638, "v5.0.0": 3755, "v5.0.0-docs": 3755, "v5.0.0rc0": 3706, "v5.0.0rc1": 3749, "v5.1.0": 3848, "v5.1.0-docs": 3848, "v5.1.0rc0": 3841, "v5.1.0rc1": 3846, "vTEST_ONLY": 3907, "vtop_level_imports-docs": 2108, "bisection": 2898}, "pages": [["", "Grid view", "Display as a agrid"], ["summarylist", "List view", "Display as a list"], ["regressions", "Show regressions", "Display information about recent regressions"]]}